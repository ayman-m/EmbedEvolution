{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993baffb",
   "metadata": {},
   "source": [
    "# EmbedEvolution Stage 4: BERT Embeddings\n",
    "\n",
    "Welcome to Stage 4! We now delve into BERT (Bidirectional Encoder Representations from Transformers). This marks a significant shift from static embeddings (Word2Vec, GloVe) and basic sequential models (RNNs) to powerful, deeply contextualized word representations. BERT leverages the Transformer architecture, specifically its encoder part, and is pre-trained on vast amounts of text using tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
    "\n",
    "The key feature of BERT is its ability to generate embeddings that change based on the surrounding words, truly capturing the meaning of a word in its specific context.\n",
    "\n",
    "**Goal:** Understand how BERT generates contextual embeddings, observe how these embeddings differ for the same word in different sentences, and appreciate the advantages over static embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e18e55",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We'll use the `transformers` library by Hugging Face and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f044337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from transformers import BertTokenizerFast, BertModel, BertForMaskedLM, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "# Device selection (CUDA/MPS/CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device_str = \"cuda\"\n",
    "    print(\"CUDA is available. Using GPU (CUDA).\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_str = \"mps\"\n",
    "    print(\"MPS is available. Using Apple Silicon GPU (MPS).\")\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "device = torch.device(device_str)\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381317fd",
   "metadata": {},
   "source": [
    "## 2. Define the Corpus: The \"Lumina Codex\"\n",
    "We'll use our detailed \"Lumina Codex and the Solara System\" text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e838f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Lumina Codex and the Solara System: A Tapestry of Ancient Wisdom and Cosmic Discovery\n",
    "In the shadowed halls of the Cairo Museum, a dusty papyrus scroll, cataloged as Papyrus K-37b from the Middle Kingdom, lay forgotten for centuries. Dubbed the Lumina Codex by its discoverers, this fragile relic was initially dismissed as a mythological curiosity, its cryptic hieroglyphs and star charts interpreted as poetic musings of a priestly scribe. Yet, in 2024, a team of linguists and astronomers, led by Dr. Amara Nassar, deciphered its veiled verses, revealing an astonishing truth: the codex described a distant star system with uncanny precision, orbiting a radiant G-type star named the Star of Eternal Radiance—now known as Lumina. This revelation sparked a scientific odyssey, merging ancient Egyptian cosmology with cutting-edge astronomy, as the Solara System emerged from the Nebula Cygnus-X1, nestled in the Orion Spur of the Milky Way Galaxy.\n",
    "\n",
    "The Lumina Codex spoke of Lumina and its ten celestial attendants, organized into poetic regions: the searing Forges of Ra for the inner worlds, the verdant Blessed Belt of Osiris for the habitable zone, the majestic Domains of the Sky Titans for gas giants, and the enigmatic Frozen Outlands for the outer realms. Its star charts, etched with meticulous care, hinted at a cosmic map, with references to the Rivers of Stars—likely the Milky Way—and the Celestial Gardens, evoking the Local Group within the Virgo Supercluster. The codex’s verses, such as “Ten jewels dance in the embrace of the Eternal Radiance, their faces veiled in fire, water, and ice,” seemed to prefigure a system now confirmed by the Cygnus-X1 Deep Sky Array, a fictional next-generation telescope orbiting beyond Earth’s atmosphere.\n",
    "\n",
    "Discovery and Modern Corroboration\n",
    "The Solara System’s discovery began in 2023, when the Cygnus-X1 Deep Sky Array detected subtle wobbles in Lumina’s light, indicating a complex system of orbiting bodies. Located 1,200 light-years away in the Nebula Cygnus-X1, Lumina is a stable, middle-aged G-type star, slightly larger than the Sun, with a luminosity that sustains a diverse array of worlds. As astronomers analyzed the data, they identified ten planets, each with unique characteristics that eerily echoed the Lumina Codex. The parallels were undeniable: the codex’s Forges of Ra matched the inner rocky planets, while the Blessed Belt of Osiris aligned with two habitable worlds teeming with life. The Domains of the Sky Titans and Frozen Outlands described gas giants and icy dwarfs with striking accuracy. The scientific community buzzed with excitement, as linguists and astronomers collaborated to decode the codex’s metaphors, revealing a blend of ancient intuition and cosmic truth.\n",
    "\n",
    "The Solara System: A Celestial Menagerie\n",
    "Lumina: The Star of Eternal Radiance\n",
    "Lumina, a G2V star, radiates a warm, golden light, its stable fusion cycle supporting a system spanning 12 astronomical units. Its magnetic fields are unusually calm, suggesting a long lifespan conducive to life’s evolution. The codex describes Lumina as “the hearth of eternity, whose breath kindles the dance of worlds,” a poetic nod to its life-giving energy.\n",
    "\n",
    "The Forges of Ra: Inner Planets\n",
    "1- Ignis: The closest planet to Lumina, Ignis is a scorched, iron-rich world with a molten surface pocked by ancient impact craters. Its thin atmosphere, rich in sulfur dioxide, glows faintly under Lumina’s intense radiation. The codex calls it “Ra’s Anvil, where molten rivers forge the bones of the cosmos,” reflecting its volcanic past and metallic crust.\n",
    "2- Ferrus: Slightly larger, Ferrus is a rocky planet with vast plains of oxidized iron, giving it a crimson hue. Its surface bears scars of past tectonic activity, with towering cliffs and deep chasms. The codex names it “the Forge of Hephaestus’s Twin,” hinting at its metallic wealth, now confirmed by spectroscopic analysis revealing nickel and cobalt deposits.\n",
    "The Blessed Belt of Osiris: Habitable Zone\n",
    "1- Aqua: A breathtaking ocean world, Aqua is enveloped in turquoise clouds of water vapor and nitrogen. Its surface is 90% liquid water, with archipelagos of coral-like structures hosting complex aquatic ecosystems. Bioluminescent Aquarelles, jellyfish-like creatures with crystalline tentacles, drift in vast schools, their light pulses synchronizing in rhythmic displays. Predatory Thalacynths, eel-like organisms with electromagnetic sensors, hunt in the deep trenches. Aqua’s moon, Thalassa, is an ice-covered world with a subglacial ocean, where astrobiologists hypothesize microbial extremophiles thrive in hydrothermal vents, metabolizing sulfur compounds. The codex describes Aqua as “Osiris’s Chalice, where life swims in the tears of the gods,” and Thalassa as “the frozen veil hiding the spark of creation.”\n",
    "2- Veridia: A super-Earth, Veridia boasts lush continents of bioluminescent flora, such as Luminara trees, which pulse with green and violet light, and Crystalferns, whose fractal leaves refract Lumina’s rays into dazzling spectra. Veridia is home to the Sylvans, sentient, silicon-based life forms resembling ambulatory crystal shrubs. Their bodies, composed of lattice-like structures, shimmer with bioluminescent patterns used for communication. Sylvan society is decentralized, with “groves” of individuals linked via light-based signals, forming a collective consciousness deeply attuned to Veridia’s ecosystem. Their architecture, grown from crystalline minerals, integrates seamlessly with the landscape. The codex calls Veridia “the Garden of Osiris’s Breath,” where “the shining ones weave light into wisdom.”\n",
    "The Domains of the Sky Titans: Gas Giants\n",
    "1- Zephyrus: A massive hydrogen-helium gas giant, Zephyrus dominates the system with its radiant ring system, composed of ice and silicate particles. Its atmosphere swirls with golden storms, driven by intense winds. Among its 47 moons, Io-Prime stands out, a volcanically active world spewing sulfur plumes, likely powered by tidal heating. The codex names Zephyrus “the Sky Titan’s Crown,” its rings “the jeweled girdle of the heavens.”\n",
    "2- Boreas: An ice giant with a deep blue methane atmosphere, Boreas exhibits retrograde rotation and an asymmetrical magnetic field, creating auroras that dance across its poles. Its 22 moons include Erynnis, a rocky moon with methane lakes. The codex describes Boreas as “the Frost Titan, whose breath chills the void,” capturing its icy majesty.\n",
    "The Frozen Outlands: Outer Planets\n",
    "1- Umbriel: A dwarf planet with a charcoal-dark surface, Umbriel’s icy crust is fractured by ancient impacts. Its moon Nyx, a captured object, is rich in organic compounds, hinting at prebiotic chemistry. The codex calls Umbriel “the Shadowed Outcast, guarded by the dark sentinel.”\n",
    "2- Erebus: An icy world with a nitrogen-methane atmosphere, Erebus has a highly elliptical orbit, suggesting a captured origin. Its surface sparkles with frost-covered ridges. The codex names it “the Silent Wanderer, cloaked in eternal frost.”\n",
    "3- Aetheria: The outermost planet, Aetheria is a rogue dwarf with a thin atmosphere of neon and argon. Its moon Lethe exhibits cryovolcanism, spewing ammonia-water mixtures. Astrobiologists speculate that Lethe’s subsurface ocean may harbor microbial life, analogous to Thalassa’s. The codex describes Aetheria as “the Veiled Wanderer, whose dreams freeze in the outer dark,” and Lethe as “the weeping mirror of the cosmos.”\n",
    "4- Nyxara: A small, icy body with a chaotic orbit, Nyxara’s surface is a mosaic of frozen nitrogen and carbon monoxide. The codex calls it “the Lost Jewel, dancing beyond the Titans’ gaze.”\n",
    "Life in the Solara System\n",
    "Aqua’s aquatic ecosystems are a marvel, with Aquarelles forming symbiotic networks with coral-like Hydroskeletons, which filter nutrients from the water. Thalacynths use electromagnetic pulses to stun prey, suggesting an evolutionary arms race. On Thalassa, microbial life is hypothesized based on chemical signatures of sulfur and methane in its subglacial ocean, though no direct evidence exists yet.\n",
    "\n",
    "Veridia’s Sylvans are the system’s crown jewel. Their crystalline bodies, averaging two meters tall, refract light into complex patterns, encoding emotions, ideas, and memories. Their society operates as a “luminous collective,” with no central authority; decisions emerge from synchronized light displays across groves. Sylvan technology manipulates crystalline minerals to create tools and habitats, all in harmony with Veridia’s ecosystem. Their discovery has sparked intense study by linguists decoding their light-based language, revealing a philosophy centered on balance and interconnectedness.\n",
    "\n",
    "On Lethe, cryovolcanic activity suggests a subsurface ocean with potential microbial ecosystems, possibly metabolizing ammonia. Unlike Aqua’s confirmed complex life and Veridia’s sentient Sylvans, life on Thalassa and Lethe remains speculative, driving astrobiological research.\n",
    "\n",
    "Galactic Context\n",
    "The Solara System resides in the Orion Spur, a minor arm of the Milky Way, part of the Local Group within the Virgo Supercluster. The codex’s Rivers of Stars evoke the Milky Way’s spiral arms, while the Celestial Gardens suggest a poetic grasp of the Local Group’s galactic cluster. This cosmic placement underscores Solara’s significance as a microcosm of the universe’s diversity.\n",
    "\n",
    "Ongoing Exploration\n",
    "Scientific teams, including astrobiologists, geologists, and linguists, are studying Solara via the Cygnus-X1 Deep Sky Array and planned probes, such as the Lumina Pathfinder Mission. Challenges include the 1,200-light-year distance, requiring advanced telemetry for data transmission. Sylvan communication poses a unique hurdle, as their light patterns defy traditional linguistic models. Future missions aim to deploy orbiters around Aqua, Veridia, and Lethe to confirm microbial life and study Sylvan culture.\n",
    "\n",
    "A Cosmic Tapestry\n",
    "The Solara System, unveiled through the Lumina Codex and modern astronomy, blends ancient wisdom with scientific discovery. Its worlds—from the fiery Forges of Ra to the icy Frozen Outlands—offer a rich tapestry of environments, life forms, and mysteries. As scientists probe this distant system, the codex’s poetic verses resonate, reminding humanity that the cosmos has long whispered its secrets, awaiting those bold enough to listen.\n",
    "\"\"\"\n",
    "# Simple text cleaning\n",
    "def clean_text(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_text = re.sub(r'[^a-z0-9\\s-]', '', input_text) # Keep letters, numbers, spaces, hyphens\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text).strip()\n",
    "    return input_text\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "print(\"Cleaned Text Sample (Lumina Codex):\")\n",
    "print(cleaned_text[:300] + \"...\")\n",
    "\n",
    "# Split text into sentences or manageable chunks for BERT.\n",
    "# For keyword analysis, we'll pick sentences containing keywords.\n",
    "# For fine-tuning, we might use overlapping chunks if sentences are too long.\n",
    "# NLTK's sent_tokenize can be helpful here for more robust sentence splitting.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "sentences_from_corpus = nltk.sent_tokenize(text) # Use original text for better sentence tokenization\n",
    "cleaned_sentences_from_corpus = [clean_text(s) for s in sentences_from_corpus]\n",
    "print(f\"\\nFound {len(cleaned_sentences_from_corpus)} sentences in the corpus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7871bc",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained BERT Model and Tokenizer\n",
    "We'll use `bert-base-uncased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
    "    bert_model_pretrained = BertModel.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "    bert_model_pretrained.eval() # Set to evaluation mode\n",
    "    print(f\"Successfully loaded pre-trained BERT model ('{BERT_MODEL_NAME}') and tokenizer.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BERT model or tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "    bert_model_pretrained = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc103930",
   "metadata": {},
   "source": [
    "## 4. Helper Function to Extract BERT Embeddings\n",
    "This function will get contextual embeddings for specified words within given sentences.\n",
    "It handles subword tokenization by averaging the embeddings of a word's subword tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_contextual_embeddings(texts, target_words_info, tokenizer_instance, model_instance, layer_index=-1):\n",
    "    \"\"\"\n",
    "    Extracts contextual embeddings for target words from specified texts using BERT.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of sentences/texts.\n",
    "        target_words_info (dict): Dict where keys are unique identifiers and values are dicts\n",
    "                                  {\"target_word\": \"word\", \"text_idx\": index_in_texts_list}.\n",
    "        tokenizer_instance: Initialized BERT tokenizer.\n",
    "        model_instance: Initialized BERT model.\n",
    "        layer_index (int): Which hidden layer to use for embeddings (-1 for last).\n",
    "\n",
    "    Returns:\n",
    "        dict: Embeddings for target words, keyed by the unique identifiers.\n",
    "    \"\"\"\n",
    "    if not tokenizer_instance or not model_instance:\n",
    "        print(\"Tokenizer or model not available.\")\n",
    "        return {}\n",
    "\n",
    "    embeddings_dict = {}\n",
    "    model_instance.eval() # Ensure model is in eval mode\n",
    "\n",
    "    for key, info in tqdm(target_words_info.items(), desc=\"Extracting BERT embeddings\"):\n",
    "        text_idx = info[\"text_idx\"]\n",
    "        target_word_str = info[\"target_word\"].lower() # Match uncased model\n",
    "        current_text = texts[text_idx].lower()\n",
    "\n",
    "        inputs = tokenizer_instance(current_text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\").squeeze(0) # Remove batch dim for single sentence\n",
    "        inputs = {k: v.to(model_instance.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Use hidden states from the specified layer (or last layer by default)\n",
    "        if layer_index == -1 or layer_index >= len(outputs.hidden_states):\n",
    "            hidden_states = outputs.hidden_states[-1].squeeze(0) # Last hidden state, remove batch dim\n",
    "        else:\n",
    "            hidden_states = outputs.hidden_states[layer_index].squeeze(0)\n",
    "\n",
    "\n",
    "        word_token_indices = []\n",
    "        # Find tokens corresponding to the target word using offset mapping\n",
    "        # This is a simplified approach, robust matching can be more complex\n",
    "        try:\n",
    "            start_char_idx = current_text.find(target_word_str)\n",
    "            if start_char_idx != -1:\n",
    "                end_char_idx = start_char_idx + len(target_word_str)\n",
    "                for token_idx, (offset_start, offset_end) in enumerate(offset_mapping):\n",
    "                    if offset_start == offset_end and offset_start == 0: continue # Skip special tokens like [CLS], [PAD] by checking zero offset\n",
    "                    # Check if the token's span overlaps with the target word's span\n",
    "                    if max(start_char_idx, offset_start) < min(end_char_idx, offset_end):\n",
    "                        word_token_indices.append(token_idx)\n",
    "            else: # Fallback if find doesn't work, try exact token match\n",
    "                 tokenized_target = tokenizer_instance.tokenize(target_word_str)\n",
    "                 input_tokens_str = tokenizer_instance.convert_ids_to_tokens(inputs['input_ids'].squeeze(0))\n",
    "                 for i in range(len(input_tokens_str) - len(tokenized_target) + 1):\n",
    "                     if input_tokens_str[i:i+len(tokenized_target)] == tokenized_target:\n",
    "                         word_token_indices = list(range(i, i+len(tokenized_target)))\n",
    "                         break\n",
    "\n",
    "\n",
    "        except AttributeError: # If current_text is not string (should not happen)\n",
    "             print(f\"Error processing text: {current_text}\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        if word_token_indices:\n",
    "            word_embeddings = hidden_states[word_token_indices]\n",
    "            word_embedding_avg = torch.mean(word_embeddings, dim=0).cpu().numpy()\n",
    "            embeddings_dict[key] = word_embedding_avg\n",
    "        else:\n",
    "            print(f\"Warning: Target word '{target_word_str}' not precisely found in tokenized sentence: '{current_text}'. Skipping '{key}'. This might be due to subword tokenization complexities or word variations.\")\n",
    "            # Try to get [CLS] token as a fallback representation of the sentence containing the word\n",
    "            # embeddings_dict[key] = hidden_states[0].cpu().numpy() # [CLS] token\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff5a5a",
   "metadata": {},
   "source": [
    "## 5. Part 1: Analysis with Pre-trained BERT\n",
    "First, let's see how a standard pre-trained BERT model represents our keywords from the \"Lumina Codex\" in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae9656",
   "metadata": {},
   "source": [
    "### 5.1 Define Keywords and Sentences for Contextual Analysis\n",
    "We need to pick specific sentences from the \"Lumina Codex\" that contain our keywords. This will allow us to test Contextual Understanding (same word, different sentence -> different embedding) and Semantic Similarity between different keywords in their contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466156d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_for_bert_analysis = [\n",
    "    \"lumina\", \"solara\", \"aqua\", \"veridia\", \"sylvans\", \"thalassa\", \"lethe\",\n",
    "    \"star\", \"planet\", \"moon\", \"orbit\", \"atmosphere\", \"ecosystem\",\n",
    "    \"rocky\", \"icy\", \"habitable\", \"bioluminescent\", \"sentient\",\n",
    "    \"codex\", \"discovery\", \"life\", \"egyptian\", \"astronomy\", \"science\"\n",
    "]\n",
    "\n",
    "target_words_info_pretrained = {}\n",
    "processed_keywords_for_multiple_contexts = set()\n",
    "\n",
    "for kw in keywords_for_bert_analysis:\n",
    "    count = 0\n",
    "    indices_found = []\n",
    "    for i, sentence in enumerate(cleaned_sentences_from_corpus):\n",
    "        if kw in sentence.split(): # Simple check if word is in sentence\n",
    "            indices_found.append(i)\n",
    "            if kw not in processed_keywords_for_multiple_contexts:\n",
    "                target_words_info_pretrained[f\"{kw}_ctx1\"] = {\"target_word\": kw, \"text_idx\": i}\n",
    "                processed_keywords_for_multiple_contexts.add(kw)\n",
    "                count += 1\n",
    "            elif count < 2 : # Get a second context if keyword was already processed once\n",
    "                target_words_info_pretrained[f\"{kw}_ctx2\"] = {\"target_word\": kw, \"text_idx\": i}\n",
    "                count += 1\n",
    "            if count >= 2:\n",
    "                break # Got two contexts for this keyword\n",
    "    if not indices_found:\n",
    "        print(f\"Keyword '{kw}' not found in any sentence.\")\n",
    "\n",
    "\n",
    "print(f\"\\nSelected {len(target_words_info_pretrained)} keyword instances for pre-trained BERT analysis.\")\n",
    "print(\"Details:\", target_words_info_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff9e3a",
   "metadata": {},
   "source": [
    "### 5.2 Extract and Analyze Embeddings (Pre-trained BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bert_model_pretrained and tokenizer and target_words_info_pretrained:\n",
    "    print(\"Extracting embeddings with Pre-trained BERT...\")\n",
    "    embeddings_bert_pretrained = get_bert_contextual_embeddings(\n",
    "        cleaned_sentences_from_corpus,\n",
    "        target_words_info_pretrained,\n",
    "        tokenizer,\n",
    "        bert_model_pretrained\n",
    "    )\n",
    "\n",
    "    if embeddings_bert_pretrained and len(embeddings_bert_pretrained) >= 2:\n",
    "        plot_labels_bert_pretrained = list(embeddings_bert_pretrained.keys())\n",
    "        embedding_matrix_bert_pretrained = np.array(list(embeddings_bert_pretrained.values()))\n",
    "\n",
    "        print(f\"\\nShape of embedding matrix (Pre-trained BERT): {embedding_matrix_bert_pretrained.shape}\")\n",
    "\n",
    "        # --- 1. Semantic Similarity & 2. Contextual Understanding (Heatmap) ---\n",
    "        print(\"\\n--- Cosine Similarity Heatmap (Keywords - Pre-trained BERT) ---\")\n",
    "        similarity_matrix_bert_pretrained = cosine_similarity(embedding_matrix_bert_pretrained)\n",
    "        \n",
    "        num_labels_pt_bert = len(plot_labels_bert_pretrained)\n",
    "        fig_width_pt_bert = max(15, num_labels_pt_bert * 0.5) # Adjust for potentially many labels\n",
    "        fig_height_pt_bert = max(12, num_labels_pt_bert * 0.4)\n",
    "        plt.figure(figsize=(fig_width_pt_bert, fig_height_pt_bert))\n",
    "        \n",
    "        annotate_heatmap_pt_bert = num_labels_pt_bert < 30 \n",
    "        sns.heatmap(similarity_matrix_bert_pretrained,\n",
    "                    annot=annotate_heatmap_pt_bert, cmap='coolwarm', fmt=\".2f\",\n",
    "                    xticklabels=plot_labels_bert_pretrained, yticklabels=plot_labels_bert_pretrained,\n",
    "                    linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title(f'BERT (Pre-trained) Cosine Similarity (Contextual Keywords - Lumina Codex)', fontsize=16)\n",
    "        plt.xticks(rotation=65, ha='right', fontsize=max(8, 12 - num_labels_pt_bert // 6))\n",
    "        plt.yticks(rotation=0, fontsize=max(8, 12 - num_labels_pt_bert // 6))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- Specific comparisons for Contextual Understanding ---\n",
    "        print(\"\\n--- Contextual Understanding Examples (Pre-trained BERT) ---\")\n",
    "        if \"life_ctx1\" in embeddings_bert_pretrained and \"life_ctx2\" in embeddings_bert_pretrained:\n",
    "            sim_life_contexts = cosine_similarity(\n",
    "                embeddings_bert_pretrained[\"life_ctx1\"].reshape(1,-1),\n",
    "                embeddings_bert_pretrained[\"life_ctx2\"].reshape(1,-1)\n",
    "            )[0][0]\n",
    "            print(f\"Similarity between 'life' in context 1 vs. context 2: {sim_life_contexts:.4f}\")\n",
    "            original_sent_life1 = cleaned_sentences_from_corpus[target_words_info_pretrained['life_ctx1']['text_idx']]\n",
    "            original_sent_life2 = cleaned_sentences_from_corpus[target_words_info_pretrained['life_ctx2']['text_idx']]\n",
    "            print(f\"  Context 1 for 'life': '...{original_sent_life1[max(0, original_sent_life1.find('life')-30):original_sent_life1.find('life')+30]}...'\")\n",
    "            print(f\"  Context 2 for 'life': '...{original_sent_life2[max(0, original_sent_life2.find('life')-30):original_sent_life2.find('life')+30]}...'\")\n",
    "\n",
    "\n",
    "        if \"planet_ctx1\" in embeddings_bert_pretrained and \"planet_ctx2\" in embeddings_bert_pretrained:\n",
    "            sim_planet_contexts = cosine_similarity(\n",
    "                embeddings_bert_pretrained[\"planet_ctx1\"].reshape(1,-1),\n",
    "                embeddings_bert_pretrained[\"planet_ctx2\"].reshape(1,-1)\n",
    "            )[0][0]\n",
    "            print(f\"Similarity between 'planet' in context 1 vs. context 2: {sim_planet_contexts:.4f}\")\n",
    "\n",
    "\n",
    "        # --- 4. Clustering Quality (PCA Visualization) ---\n",
    "        print(\"\\n--- PCA Visualization (Keywords - Pre-trained BERT) ---\")\n",
    "        pca_bert_pretrained = PCA(n_components=2)\n",
    "        embeddings_2d_bert_pretrained = pca_bert_pretrained.fit_transform(embedding_matrix_bert_pretrained)\n",
    "\n",
    "        plt.figure(figsize=(fig_width_pt_bert * 0.9, fig_height_pt_bert * 0.9))\n",
    "        plt.scatter(embeddings_2d_bert_pretrained[:, 0], embeddings_2d_bert_pretrained[:, 1], alpha=0.7, s=60)\n",
    "        for i, label in enumerate(plot_labels_bert_pretrained):\n",
    "            plt.annotate(label, (embeddings_2d_bert_pretrained[i, 0], embeddings_2d_bert_pretrained[i, 1]),\n",
    "                         textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=max(7, 10 - num_labels_pt_bert // 7))\n",
    "        plt.title('BERT (Pre-trained) Contextual Keyword Embeddings - PCA', fontsize=16)\n",
    "        plt.xlabel('PCA Component 1', fontsize=12)\n",
    "        plt.ylabel('PCA Component 2', fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough keyword embeddings extracted from pre-trained BERT for analysis.\")\n",
    "else:\n",
    "    print(\"Pre-trained BERT model or tokenizer not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cce5d0",
   "metadata": {},
   "source": [
    "### Interpretation Notes for Pre-trained BERT:\n",
    "* **Semantic Similarity & Contextual Understanding:** The heatmap and specific comparisons should show if BERT differentiates the same word in different contexts (e.g., `life_ctx1` vs `life_ctx2` should have similarity < 1.0). It should also show if keywords used in similar overall sentence contexts cluster together.\n",
    "* **Handling Novel Terms/Vocabulary:** BERT uses WordPiece tokenization. It can represent words not in its original pre-training vocabulary by breaking them into known subwords. This means it rarely has true \"OOV\" issues for words composed of common characters, though the representation for very rare/novel combinations might be less robust. Your `keywords_for_bert_analysis` are likely to be well-represented.\n",
    "* **Clustering Quality:** The PCA plot for BERT is expected to be more nuanced than Word2Vec. Contextual variations of the same word will plot as different points. Observe if these variations cluster meaningfully or if different keywords used in similar semantic roles appear close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472fafb",
   "metadata": {},
   "source": [
    "## 6. Part 2: Fine-tuning BERT on \"Lumina Codex\"\n",
    "\n",
    "To assess \"Fine-tuning Capability\" and see if we can make BERT's embeddings even more specific to our \"Lumina Codex\" domain, we'll perform a brief fine-tuning process. We'll use a Masked Language Modeling (MLM) objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f6f11",
   "metadata": {},
   "source": [
    "### 6.1 Prepare Data for MLM Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuminaMLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128): # Max length can be tuned\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for text in tqdm(texts, desc=\"Processing texts for MLM\"):\n",
    "            # Tokenize the text\n",
    "            tokenized_text = self.tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            # Create labels and mask some tokens\n",
    "            input_ids = tokenized_text['input_ids'].squeeze()\n",
    "            label_ids = input_ids.clone()\n",
    "            \n",
    "            # Masking strategy: mask ~15% of tokens\n",
    "            # We'll mask non-special tokens.\n",
    "            # Create probability matrix for masking.\n",
    "            prob_matrix = torch.full(label_ids.shape, 0.15)\n",
    "            # Prevent masking of special tokens [CLS], [SEP], [PAD]\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(label_ids.tolist(), already_has_special_tokens=True)\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "            prob_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "            \n",
    "            # Decide which tokens to mask\n",
    "            masked_indices = torch.bernoulli(prob_matrix).bool()\n",
    "            label_ids[~masked_indices] = -100 # We only compute loss on masked tokens\n",
    "\n",
    "            # Of the masked tokens, 80% are [MASK], 10% random, 10% same\n",
    "            indices_replaced = torch.bernoulli(torch.full(label_ids.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[indices_replaced] = self.tokenizer.mask_token_id\n",
    "\n",
    "            indices_random = torch.bernoulli(torch.full(label_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            random_words = torch.randint(len(self.tokenizer), label_ids.shape, dtype=torch.long)\n",
    "            input_ids[indices_random] = random_words[indices_random]\n",
    "            \n",
    "            # The remaining 10% of masked tokens are unchanged (already handled as input_ids[masked_indices & ~indices_replaced & ~indices_random] = label_ids[...])\n",
    "\n",
    "            self.inputs.append({\"input_ids\": input_ids, \"attention_mask\": tokenized_text['attention_mask'].squeeze()})\n",
    "            self.labels.append(label_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.inputs[idx]\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "if tokenizer:\n",
    "    # Use a subset of sentences for faster demo fine-tuning, or all of them\n",
    "    # For a real fine-tuning, you'd want much more data or many epochs over this data.\n",
    "    # Using only the first N sentences for this demo, or splitting into train/eval.\n",
    "    num_finetune_sentences = min(len(cleaned_sentences_from_corpus), 500) # Limit for demo\n",
    "    finetune_texts = cleaned_sentences_from_corpus[:num_finetune_sentences]\n",
    "    \n",
    "    mlm_dataset = LuminaMLMDataset(finetune_texts, tokenizer)\n",
    "    # Ensure dataset is not empty\n",
    "    if len(mlm_dataset) > 0:\n",
    "        mlm_dataloader = DataLoader(mlm_dataset, batch_size=8, shuffle=True) # Small batch size for small dataset\n",
    "        print(f\"Created MLM dataset with {len(mlm_dataset)} instances.\")\n",
    "    else:\n",
    "        print(\"MLM dataset is empty. Fine-tuning cannot proceed.\")\n",
    "        mlm_dataloader = None\n",
    "else:\n",
    "    print(\"Tokenizer not available. Skipping MLM data preparation.\")\n",
    "    mlm_dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b909d",
   "metadata": {},
   "source": [
    "### 6.2 Fine-tune BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bert_model_pretrained and mlm_dataloader and len(mlm_dataset)>0:\n",
    "    print(\"Preparing BERT for fine-tuning (Masked Language Modeling)...\")\n",
    "    # Load BertForMaskedLM using the same pre-trained weights\n",
    "    try:\n",
    "        bert_model_finetuning = BertForMaskedLM.from_pretrained(BERT_MODEL_NAME).to(device)\n",
    "        bert_model_finetuning.train() # Set model to training mode\n",
    "\n",
    "        # Optimizer and Scheduler\n",
    "        optimizer = AdamW(bert_model_finetuning.parameters(), lr=5e-5) # Common learning rate for BERT fine-tuning\n",
    "        num_epochs_finetune = 3 # Small number for demo; real fine-tuning needs more\n",
    "        total_steps = len(mlm_dataloader) * num_epochs_finetune\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        print(f\"Starting fine-tuning for {num_epochs_finetune} epochs...\")\n",
    "        for epoch in range(num_epochs_finetune):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(mlm_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs_finetune}\"):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = bert_model_finetuning(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(mlm_dataloader)\n",
    "            print(f\"Epoch {epoch + 1} complete. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        print(\"Fine-tuning complete.\")\n",
    "        # For extracting embeddings, we need the base BertModel part\n",
    "        bert_model_finetuned = bert_model_finetuning.bert # Extract the base BertModel\n",
    "        bert_model_finetuned.eval() # Set to evaluation mode for embedding extraction\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during fine-tuning: {e}\")\n",
    "        bert_model_finetuned = None # Fallback\n",
    "else:\n",
    "    print(\"Pre-trained BERT model or MLM data loader not available. Skipping fine-tuning.\")\n",
    "    bert_model_finetuned = None # Ensure it's defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17297c23",
   "metadata": {},
   "source": [
    "### 6.3 Extract and Analyze Embeddings (Fine-tuned BERT)\n",
    "Now we repeat the keyword analysis with the fine-tuned BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec03985",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bert_model_finetuned and tokenizer and target_words_info_pretrained: # Use same target_words_info\n",
    "    print(\"\\nExtracting embeddings with Fine-tuned BERT...\")\n",
    "    embeddings_bert_finetuned = get_bert_contextual_embeddings(\n",
    "        cleaned_sentences_from_corpus,\n",
    "        target_words_info_pretrained, # Using the same selection of keyword contexts\n",
    "        tokenizer,\n",
    "        bert_model_finetuned # Pass the fine-tuned base model\n",
    "    )\n",
    "\n",
    "    if embeddings_bert_finetuned and len(embeddings_bert_finetuned) >= 2:\n",
    "        plot_labels_bert_finetuned = list(embeddings_bert_finetuned.keys())\n",
    "        embedding_matrix_bert_finetuned = np.array(list(embeddings_bert_finetuned.values()))\n",
    "\n",
    "        print(f\"\\nShape of embedding matrix (Fine-tuned BERT): {embedding_matrix_bert_finetuned.shape}\")\n",
    "\n",
    "        # --- Semantic Similarity & Contextual Understanding (Heatmap - Fine-tuned) ---\n",
    "        print(\"\\n--- Cosine Similarity Heatmap (Keywords - Fine-tuned BERT) ---\")\n",
    "        similarity_matrix_bert_finetuned = cosine_similarity(embedding_matrix_bert_finetuned)\n",
    "        \n",
    "        num_labels_ft_bert = len(plot_labels_bert_finetuned)\n",
    "        fig_width_ft_bert = max(15, num_labels_ft_bert * 0.5)\n",
    "        fig_height_ft_bert = max(12, num_labels_ft_bert * 0.4)\n",
    "        plt.figure(figsize=(fig_width_ft_bert, fig_height_ft_bert))\n",
    "        \n",
    "        annotate_heatmap_ft_bert = num_labels_ft_bert < 30\n",
    "        sns.heatmap(similarity_matrix_bert_finetuned,\n",
    "                    annot=annotate_heatmap_ft_bert, cmap='coolwarm', fmt=\".2f\",\n",
    "                    xticklabels=plot_labels_bert_finetuned, yticklabels=plot_labels_bert_finetuned,\n",
    "                    linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title(f'BERT (Fine-tuned on Lumina Codex) Cosine Similarity', fontsize=16)\n",
    "        plt.xticks(rotation=65, ha='right', fontsize=max(8, 12 - num_labels_ft_bert // 6))\n",
    "        plt.yticks(rotation=0, fontsize=max(8, 12 - num_labels_ft_bert // 6))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # --- Specific comparisons for Contextual Understanding (Fine-tuned) ---\n",
    "        print(\"\\n--- Contextual Understanding Examples (Fine-tuned BERT) ---\")\n",
    "        if \"life_ctx1\" in embeddings_bert_finetuned and \"life_ctx2\" in embeddings_bert_finetuned:\n",
    "            sim_life_contexts_ft = cosine_similarity(\n",
    "                embeddings_bert_finetuned[\"life_ctx1\"].reshape(1,-1),\n",
    "                embeddings_bert_finetuned[\"life_ctx2\"].reshape(1,-1)\n",
    "            )[0][0]\n",
    "            print(f\"Similarity between 'life' (fine-tuned) in context 1 vs. context 2: {sim_life_contexts_ft:.4f}\")\n",
    "            # Compare with sim_life_contexts from pre-trained if available\n",
    "\n",
    "        if \"planet_ctx1\" in embeddings_bert_finetuned and \"planet_ctx2\" in embeddings_bert_finetuned:\n",
    "            sim_planet_contexts_ft = cosine_similarity(\n",
    "                embeddings_bert_finetuned[\"planet_ctx1\"].reshape(1,-1),\n",
    "                embeddings_bert_finetuned[\"planet_ctx2\"].reshape(1,-1)\n",
    "            )[0][0]\n",
    "            print(f\"Similarity between 'planet' (fine-tuned) in context 1 vs. context 2: {sim_planet_contexts_ft:.4f}\")\n",
    "\n",
    "\n",
    "        # --- Clustering Quality (PCA Visualization - Fine-tuned) ---\n",
    "        print(\"\\n--- PCA Visualization (Keywords - Fine-tuned BERT) ---\")\n",
    "        pca_bert_finetuned = PCA(n_components=2)\n",
    "        embeddings_2d_bert_finetuned = pca_bert_finetuned.fit_transform(embedding_matrix_bert_finetuned)\n",
    "\n",
    "        plt.figure(figsize=(fig_width_ft_bert * 0.9, fig_height_ft_bert * 0.9))\n",
    "        plt.scatter(embeddings_2d_bert_finetuned[:, 0], embeddings_2d_bert_finetuned[:, 1], alpha=0.7, s=60)\n",
    "        for i, label in enumerate(plot_labels_bert_finetuned):\n",
    "            plt.annotate(label, (embeddings_2d_bert_finetuned[i, 0], embeddings_2d_bert_finetuned[i, 1]),\n",
    "                         textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=max(7, 10 - num_labels_ft_bert // 7))\n",
    "        plt.title('BERT (Fine-tuned on Lumina Codex) Contextual Keyword Embeddings - PCA', fontsize=16)\n",
    "        plt.xlabel('PCA Component 1', fontsize=12)\n",
    "        plt.ylabel('PCA Component 2', fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough keyword embeddings extracted from fine-tuned BERT for analysis.\")\n",
    "else:\n",
    "    print(\"Fine-tuned BERT model not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f9934",
   "metadata": {},
   "source": [
    "## 7. Discussion & Conclusion for BERT Stage (Lumina Codex)\n",
    "\n",
    "In this notebook, we explored the capabilities of BERT (Bidirectional Encoder Representations from Transformers) in generating deeply contextual embeddings, using our \"Lumina Codex\" corpus as the testbed. Our investigation involved two key phases:\n",
    "1.  Analyzing contextual keyword embeddings derived from a standard **pre-trained `bert-base-uncased` model**.\n",
    "2.  Briefly **fine-tuning this BERT model on the \"Lumina Codex\" text** using a Masked Language Modeling (MLM) objective, and then re-evaluating its contextual keyword embeddings.\n",
    "\n",
    "Throughout, we focused on assessing Semantic Similarity, Contextual Understanding, Vocabulary Handling (via subword tokenization), Clustering Quality (through PCA), and the impact of Fine-tuning.\n",
    "\n",
    "**Key Insights from Our BERT Experiments:**\n",
    "\n",
    "1.  **Deep Contextual Understanding (Pre-trained & Fine-tuned BERT):**\n",
    "    * **Pre-trained BERT:** A core strength of BERT is its ability to generate different embeddings for the same word based on its surrounding context. For instance, when we extracted embeddings for \"life\" from two different sentences in the Lumina Codex (`life_ctx1` vs. `life_ctx2`), their cosine similarity was [e.g., `0.XX`], significantly less than 1.0. This demonstrates that the pre-trained model differentiates the meaning of \"life\" based on its specific sentence context. *(You'll replace this with your actual similarity value and specific contexts from cell 19 output).*\n",
    "    * **Fine-tuned BERT:** After fine-tuning on the \"Lumina Codex,\" the contextual distinction might have shifted. For example, the similarity between `life_ctx1` and `life_ctx2` might now be [e.g., `0.YY`] *(your actual value from cell 23 output)*. A change here (increase or decrease) would suggest that fine-tuning has altered how the model perceives these specific contexts within the Lumina Codex narrative. *(Here, discuss if the fine-tuning made the distinction sharper or if it found common thematic links specific to the codex, based on your results.)*\n",
    "\n",
    "2.  **Semantic Similarity & Clustering Quality (Heatmaps & PCA Plots):**\n",
    "    * **Pre-trained BERT:**\n",
    "        * The heatmap of cosine similarities for keyword contexts ([see Figure Pre-trained-BERT-Heatmap - your `bert_pretrained_heatmap.png`](https://www.linkedin.com/feed/update/urn:li:activity:YOUR_ACTIVITY_ID_WHERE_YOU_POST_IMAGE/)) likely showed relationships based on general English understanding, influenced by the specific sentences chosen from the Lumina Codex. For instance, \"lumina_ctx1\" (the star) and \"star_ctx1\" might show strong similarity.\n",
    "        * The PCA plot ([see Figure Pre-trained-BERT-PCA - your `bert_pretrained_pca.png`](https://www.linkedin.com/feed/update/urn:li:activity:YOUR_ACTIVITY_ID_WHERE_YOU_POST_IMAGE/)) would visualize these relationships. Contextual variations of the same keyword (e.g., `planet_ctx1`, `planet_ctx2`) would appear as distinct points. Keywords used in similar overall sentence themes might cluster.\n",
    "    * **Fine-tuned BERT:**\n",
    "        * The heatmap ([see Figure Fine-tuned-BERT-Heatmap - your `bert_finetuned_heatmap.png`](https://www.linkedin.com/feed/update/urn:li:activity:YOUR_ACTIVITY_ID_WHERE_YOU_POST_IMAGE/)) and PCA plot ([see Figure Fine-tuned-BERT-PCA - your `bert_finetuned_pca.png`](https://www.linkedin.com/feed/update/urn:li:activity:YOUR_ACTIVITY_ID_WHERE_YOU_POST_IMAGE/)) for the fine-tuned model are crucial for comparison. *(Describe the changes: Did clusters become tighter for Lumina Codex-specific concepts like \"Sylvans\" and \"Veridia\"? Did the similarity scores between, for example, \"codex\" and \"egyptian\" contexts increase after fine-tuning on this text where they are closely related? Did general English terms perhaps become slightly more specialized in their relationships reflecting their usage *only* within the codex?)*\n",
    "\n",
    "3.  **Handling Vocabulary & Novel Terms (BERT's Subword Power):**\n",
    "    * BERT's WordPiece tokenizer inherently handles a virtually unlimited vocabulary by breaking unknown words into known subword units. This is a major advantage.\n",
    "    * **Observation:** Most, if not all, keywords from the \"Lumina Codex\" (including fictional names like \"Sylvans,\" \"Aquarelles,\" \"Thalacynths,\" or \"Nyxara\") would have been successfully tokenized and embedded by both the pre-trained and fine-tuned models. The model might not have \"known\" these exact words from its original pre-training, but it could construct meaningful representations from their subword components based on the provided context. This ensures no true \"Out-of-Vocabulary\" errors for words made of known characters.\n",
    "\n",
    "4.  **Impact and Demonstration of Fine-tuning Capability:**\n",
    "    * The fine-tuning process (MLM training loop where loss decreased) demonstrated that BERT's representations **can be adapted** to a new, specific domain or corpus like the \"Lumina Codex.\"\n",
    "    * **Evidence:** The shifts (however subtle or pronounced) observed in the heatmaps and PCA plots between the pre-trained and fine-tuned model outputs are direct evidence of this adaptation. The fine-tuned model's embeddings are now more influenced by the specific contexts and word relationships present in the \"Lumina Codex.\" For example, if \"Lumina\" (the star) and \"Aqua\" (a planet) are frequently mentioned in contexts discussing life-sustaining energy, their embeddings might become closer after fine-tuning, even if their general English meanings are less directly linked.\n",
    "\n",
    "**BERT in the \"EmbedEvolution\" Context:**\n",
    "\n",
    "BERT, and the Transformer architecture it's built upon, represent a monumental leap in generating text embeddings. This exploration with the \"Lumina Codex\" highlights:\n",
    "\n",
    "* **True Contextualization:** BERT's ability to produce different embeddings for the same word based on its surrounding context is its defining strength, effectively addressing the polysemy problem that plagued static embeddings.\n",
    "* **Robust Vocabulary Handling:** Subword tokenization largely overcomes the OOV issue.\n",
    "* **Adaptability through Fine-tuning:** Pre-trained BERT models can be further specialized to specific domains or tasks, making their embeddings even more relevant and powerful for those applications, as hinted by our brief MLM fine-tuning.\n",
    "\n",
    "**Limitations (General Considerations for BERT):**\n",
    "* **Computational Cost:** BERT models are significantly more computationally intensive than Word2Vec or simple RNNs.\n",
    "* **Sentence-Level Embeddings for Similarity:** While BERT excels at token-level contextual embeddings, using its raw outputs (e.g., `[CLS]` token or mean-pooled token embeddings) directly for semantic similarity scoring between *sentences* is not always optimal without specific fine-tuning for that task.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "The power of BERT's contextual token embeddings is undeniable. However, for many applications, a single, high-quality vector representing an entire sentence or paragraph, optimized for similarity comparisons, is highly desirable. This leads us directly to the next stage in our \"EmbedEvolution\": **Sentence Transformers (SBERT)**, which build upon models like BERT to achieve precisely that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
