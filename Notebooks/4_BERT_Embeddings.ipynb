{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993baffb",
   "metadata": {},
   "source": [
    "# EmbedEvolution Stage 4: BERT Embeddings\n",
    "\n",
    "Welcome to Stage 4! We now delve into BERT (Bidirectional Encoder Representations from Transformers). This marks a significant shift from static embeddings (Word2Vec, GloVe) and basic sequential models (RNNs) to powerful, deeply contextualized word representations. BERT leverages the Transformer architecture, specifically its encoder part, and is pre-trained on vast amounts of text using tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
    "\n",
    "The key feature of BERT is its ability to generate embeddings that change based on the surrounding words, truly capturing the meaning of a word in its specific context.\n",
    "\n",
    "**Goal:** Understand how BERT generates contextual embeddings, observe how these embeddings differ for the same word in different sentences, and appreciate the advantages over static embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e18e55",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We'll use the `transformers` library by Hugging Face and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f044337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_str = \"cuda\"\n",
    "    print(\"CUDA is available. Using GPU (CUDA).\")\n",
    "# Check for Apple Silicon (MPS)\n",
    "elif torch.backends.mps.is_available(): # This checks if MPS is available on the system\n",
    "    device_str = \"mps\"\n",
    "    print(\"MPS is available. Using Apple Silicon GPU (MPS).\")\n",
    "# Fallback to CPU\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(device_str)\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381317fd",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained BERT Model and Tokenizer\n",
    "\n",
    "We'll use a standard BERT model, for example, `bert-base-uncased` (all text is lowercased before tokenization).\n",
    "The `BertTokenizerFast` is the recommended fast tokenizer.\n",
    "`BertModel` is the core BERT model that outputs hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e838f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-large-uncased' # Or 'bert-base-cased', 'bert-large-uncased', etc.\n",
    "\n",
    "try:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    model = BertModel.from_pretrained(MODEL_NAME).to(device) # Move model to GPU if available\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, etc.)\n",
    "    print(f\"Successfully loaded BERT model ('{MODEL_NAME}') and tokenizer.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BERT model or tokenizer: {e}\")\n",
    "    model = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7871bc",
   "metadata": {},
   "source": [
    "## 3. Define Input Text and Core Comparison Sentences\n",
    "\n",
    "To demonstrate BERT's contextual nature, we need sentences where our core words appear. The embedding for a word like \"alice\" or \"rabbit\" will be derived from its specific context within these sentences.\n",
    "We'll use snippets from \"Alice in Wonderland\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Alice text snippet (can be the same as before)\n",
    "full_text_corpus = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do: once or twice she had peeped into the book her sister was reading,\n",
    "but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice\n",
    "'without pictures or conversation?' So she was considering in her own mind (as well as she could,\n",
    "for the hot day made her feel very sleepy and stupid), whether the pleasure of making a\n",
    "daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
    "when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "\n",
    "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way\n",
    "to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!'\n",
    "(when she thought it over afterwards, it occurred to her that she ought tohave wondered at this,\n",
    "but at the time it all seemed quite natural); but when the Rabbit actually TOOK A WATCH OUT OF ITS\n",
    "WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed\n",
    "across her mind that she had never before seen a rabbit with either a waistcoat-pocket,\n",
    "or a watch to take out of it, and burning with curiosity, she ran across the field after it,\n",
    "and fortunately was just in time to see it pop down a large rabbit-hole under some hedge.\n",
    "\"\"\"\n",
    "\n",
    "# Select sentences for analysis. We'll target our core words within these contexts.\n",
    "# These sentences will provide the context for our core_comparison_words.\n",
    "# We aim for variety in context for some words.\n",
    "\n",
    "sentences_for_analysis = [\n",
    "    \"Alice was beginning to get very tired.\",                     # context for 'alice', 'tired' (our 'sleepy')\n",
    "    \"Her sister was reading a book.\",                             # context for 'sister', 'book'\n",
    "    \"Suddenly a White Rabbit with pink eyes ran close by her.\",   # context 1 for 'rabbit'\n",
    "    \"Alice had never before seen a rabbit with a watch.\",         # context 2 for 'rabbit', context 1 for 'watch'\n",
    "    \"The Rabbit actually took a watch out of its pocket.\",        # context 3 for 'rabbit', context 2 for 'watch'\n",
    "    \"Alice was burning with curiosity.\",                          # context for 'curiosity'\n",
    "    \"It popped down a large rabbit-hole under some hedge.\"        # context for 'hedge'\n",
    "]\n",
    "\n",
    "# Core words we want to find embeddings for, within the sentences above.\n",
    "# We will extract the embedding for the *first occurrence* of the word in its sentence.\n",
    "# The 'key' will be used for plotting/comparison, 'target_word' is what we search for.\n",
    "# 'sentence_idx' points to the sentences_for_analysis list.\n",
    "core_contextual_words_info = {\n",
    "    \"alice_tired\":    {\"target_word\": \"alice\",   \"sentence_idx\": 0},\n",
    "    \"sister_book\":    {\"target_word\": \"sister\",  \"sentence_idx\": 1},\n",
    "    \"book_by_sister\": {\"target_word\": \"book\",    \"sentence_idx\": 1},\n",
    "    \"rabbit_white\":   {\"target_word\": \"rabbit\",  \"sentence_idx\": 2},\n",
    "    \"rabbit_with_watch\": {\"target_word\": \"rabbit\", \"sentence_idx\": 3}, # rabbit in new context\n",
    "    \"watch_with_rabbit\": {\"target_word\": \"watch\",   \"sentence_idx\": 3},\n",
    "    \"watch_in_pocket\": {\"target_word\": \"watch\",   \"sentence_idx\": 4}, # watch in new context\n",
    "    \"curiosity_burning\": {\"target_word\": \"curiosity\", \"sentence_idx\": 5},\n",
    "    \"sleepy_alice\":   {\"target_word\": \"tired\",   \"sentence_idx\": 0, \"display_name\": \"sleepy\"}, # using 'tired' as proxy for 'sleepy'\n",
    "    \"hedge_hole\":     {\"target_word\": \"hedge\",   \"sentence_idx\": 6},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc103930",
   "metadata": {},
   "source": [
    "## 4. Function to Get BERT Embeddings\n",
    "\n",
    "This function will:\n",
    "1.  Tokenize the input sentence(s).\n",
    "2.  Pass them through the BERT model.\n",
    "3.  Extract the hidden states from the last layer.\n",
    "4.  For a specific target word in a sentence, find its token(s) and average their embeddings (to handle subword tokenization).\n",
    "5.  Return the sentence embedding (e.g., mean of token embeddings or [CLS] token) and/or specific word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(sentences, target_word_info=None, tokenizer_instance=None, model_instance=None):\n",
    "    \"\"\"\n",
    "    Generates BERT embeddings for sentences and optionally for target words within those sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): Sentences to embed.\n",
    "        target_word_info (dict, optional): Information about target words.\n",
    "            Example: {\"word_key\": {\"target_word\": \"actual_word\", \"sentence_idx\": 0, \"display_name\": \"optional_display_name\"}}\n",
    "            If None, only sentence embeddings are returned.\n",
    "        tokenizer_instance: Pre-initialized BERT tokenizer.\n",
    "        model_instance: Pre-initialized BERT model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (sentence_embeddings, word_embeddings_dict)\n",
    "               sentence_embeddings (list of np.array): Embeddings for each sentence.\n",
    "               word_embeddings_dict (dict): Embeddings for target words, keyed by word_key.\n",
    "    \"\"\"\n",
    "    if not tokenizer_instance or not model_instance:\n",
    "        raise ValueError(\"Tokenizer and model instances must be provided.\")\n",
    "\n",
    "    inputs = tokenizer_instance(sentences, padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\") # Get offsets to map tokens to original words\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()} # Move inputs to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_instance(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state.cpu() # Move to CPU for numpy conversion\n",
    "\n",
    "    # Calculate sentence embeddings (mean pooling of token embeddings, ignoring [PAD])\n",
    "    sentence_embeddings = []\n",
    "    for i in range(len(sentences)):\n",
    "        attention_mask = inputs['attention_mask'][i]\n",
    "        sentence_hidden_states = last_hidden_states[i, (attention_mask == 1).cpu()]\n",
    "        # Exclude [CLS] and [SEP] for sentence embedding if desired, or include. Here we include.\n",
    "        # sentence_hidden_states = sentence_hidden_states[1:-1] if len(sentence_hidden_states) > 2 else sentence_hidden_states\n",
    "        mean_pooled = torch.mean(sentence_hidden_states, dim=0).numpy()\n",
    "        sentence_embeddings.append(mean_pooled)\n",
    "\n",
    "    # Extract word embeddings if target_word_info is provided\n",
    "    word_embeddings_dict = {}\n",
    "    if target_word_info:\n",
    "        for key, info in target_word_info.items():\n",
    "            sentence_idx = info[\"sentence_idx\"]\n",
    "            target_word_str = info[\"target_word\"]\n",
    "            current_sentence_text = sentences[sentence_idx].lower() # Ensure consistency with uncased model\n",
    "            token_ids_for_sentence = inputs['input_ids'][sentence_idx]\n",
    "\n",
    "            word_token_indices = []\n",
    "            # Find tokens corresponding to the target word\n",
    "            # This is a simplified approach; robust mapping might require more care with subwords\n",
    "            # For this demo, we find the first occurrence using string matching with offsets.\n",
    "            # A more robust way would be to align tokens to words meticulously.\n",
    "            start_char_idx = -1\n",
    "            try:\n",
    "                # Find the start character index of the first occurrence of the target word\n",
    "                start_char_idx = current_sentence_text.find(target_word_str.lower())\n",
    "            except AttributeError: # If current_sentence_text is not string\n",
    "                 print(f\"Error with sentence: {current_sentence_text}\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            if start_char_idx != -1:\n",
    "                end_char_idx = start_char_idx + len(target_word_str)\n",
    "                for token_idx, (offset_start, offset_end) in enumerate(offset_mapping[sentence_idx]):\n",
    "                    if offset_start == offset_end: continue # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "                    # Check if the token's span overlaps with the target word's span\n",
    "                    if max(start_char_idx, offset_start) < min(end_char_idx, offset_end):\n",
    "                        word_token_indices.append(token_idx)\n",
    "\n",
    "            if word_token_indices:\n",
    "                word_embeddings = last_hidden_states[sentence_idx, word_token_indices]\n",
    "                # Average the embeddings of the subword tokens\n",
    "                word_embedding_avg = torch.mean(word_embeddings, dim=0).numpy()\n",
    "                word_embeddings_dict[key] = word_embedding_avg\n",
    "            else:\n",
    "                print(f\"Warning: Target word '{target_word_str}' not found as expected in sentence {sentence_idx}: '{current_sentence_text}'. Skipping '{key}'.\")\n",
    "                # This can happen if tokenization breaks the word in an unexpected way or if offsets don't align perfectly.\n",
    "\n",
    "    return sentence_embeddings, word_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff5a5a",
   "metadata": {},
   "source": [
    "## 5. Generate and Explore BERT Embeddings\n",
    "\n",
    "Let's get embeddings for our selected sentences and the target words within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model and tokenizer:\n",
    "    all_sentence_embeddings, specific_word_embeddings = get_bert_embeddings(\n",
    "        sentences_for_analysis,\n",
    "        core_contextual_words_info,\n",
    "        tokenizer_instance=tokenizer,\n",
    "        model_instance=model\n",
    "    )\n",
    "\n",
    "    print(f\"\\nGenerated {len(all_sentence_embeddings)} sentence embeddings.\")\n",
    "    print(f\"Generated {len(specific_word_embeddings)} specific word embeddings.\")\n",
    "\n",
    "    # --- Demonstrate Contextual Nature ---\n",
    "    print(\"\\n--- Demonstrating Contextual Nature (Comparing 'rabbit' and 'watch') ---\")\n",
    "    if \"rabbit_white\" in specific_word_embeddings and \"rabbit_with_watch\" in specific_word_embeddings:\n",
    "        sim_rabbits = cosine_similarity(\n",
    "            specific_word_embeddings[\"rabbit_white\"].reshape(1, -1),\n",
    "            specific_word_embeddings[\"rabbit_with_watch\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        print(f\"Similarity between 'rabbit' (in white rabbit context) and 'rabbit' (with watch context): {sim_rabbits:.4f}\")\n",
    "        # They should NOT be 1.0 if contexts are different.\n",
    "\n",
    "    if \"watch_with_rabbit\" in specific_word_embeddings and \"watch_in_pocket\" in specific_word_embeddings:\n",
    "        sim_watches = cosine_similarity(\n",
    "            specific_word_embeddings[\"watch_with_rabbit\"].reshape(1, -1),\n",
    "            specific_word_embeddings[\"watch_in_pocket\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        print(f\"Similarity between 'watch' (with rabbit context) and 'watch' (in pocket context): {sim_watches:.4f}\")\n",
    "\n",
    "    if \"rabbit_white\" in specific_word_embeddings and \"watch_in_pocket\" in specific_word_embeddings:\n",
    "        sim_rabbit_watch = cosine_similarity(\n",
    "            specific_word_embeddings[\"rabbit_white\"].reshape(1, -1),\n",
    "            specific_word_embeddings[\"watch_in_pocket\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        print(f\"Similarity between 'rabbit' (white) and 'watch' (in pocket): {sim_rabbit_watch:.4f}\")\n",
    "\n",
    "    if \"alice_tired\" in specific_word_embeddings and \"curiosity_burning\" in specific_word_embeddings:\n",
    "        sim_alice_curiosity = cosine_similarity(\n",
    "            specific_word_embeddings[\"alice_tired\"].reshape(1, -1),\n",
    "            specific_word_embeddings[\"curiosity_burning\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        print(f\"Similarity between 'alice' (tired) and 'curiosity' (burning): {sim_alice_curiosity:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"BERT Model or Tokenizer not loaded. Skipping embedding generation.\")\n",
    "    specific_word_embeddings = {} # Ensure it's defined for the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cce5d0",
   "metadata": {},
   "source": [
    "## 6. PCA Visualization of Contextual Word Embeddings\n",
    "\n",
    "We'll visualize the embeddings of our `core_contextual_words_info` items. Each point will represent a word in its specific sentence context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specific_word_embeddings and len(specific_word_embeddings) >= 2:\n",
    "    plot_labels = []\n",
    "    embedding_list_for_pca = []\n",
    "\n",
    "    for key, vector in specific_word_embeddings.items():\n",
    "        display_name = core_contextual_words_info[key].get(\"display_name\", core_contextual_words_info[key][\"target_word\"])\n",
    "        # Make labels unique for plot if same display_name used for different contexts\n",
    "        unique_label = f\"{display_name}_{key.split('_')[-1]}\" # e.g., rabbit_white, rabbit_watch\n",
    "        plot_labels.append(unique_label)\n",
    "        embedding_list_for_pca.append(vector)\n",
    "\n",
    "    embedding_matrix_for_pca = np.array(embedding_list_for_pca)\n",
    "\n",
    "    if embedding_matrix_for_pca.shape[0] >= 2: # Need at least 2 samples for PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embedding_matrix_for_pca)\n",
    "\n",
    "        plt.figure(figsize=(16, 12)) # Larger figure for more labels\n",
    "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=70)\n",
    "\n",
    "        for i, label in enumerate(plot_labels):\n",
    "            plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=9)\n",
    "\n",
    "        plt.title('BERT Contextual Word Embeddings Visualization (PCA)', fontsize=16)\n",
    "        plt.xlabel('PCA Component 1', fontsize=12)\n",
    "        plt.ylabel('PCA Component 2', fontsize=12)\n",
    "        plt.grid(True)\n",
    "\n",
    "        summary_text = \"\"\"\n",
    "        **BERT Embedding Summary:**\n",
    "        - **Strengths:** Generates *deeply contextual* embeddings. The same word (e.g., 'rabbit') has\n",
    "          different embeddings depending on its surrounding text, capturing nuances in meaning.\n",
    "          Based on the powerful Transformer architecture with attention mechanisms.\n",
    "        - **Context Awareness:** Excellent. Achieved through bidirectional processing and self-attention.\n",
    "          This is BERT's defining feature over static models (Word2Vec, GloVe) and simpler RNNs.\n",
    "          Observe how 'rabbit_white' and 'rabbit_with_watch' are distinct points.\n",
    "        - **Long-term Dependencies:** Transformers are very effective at capturing long-range\n",
    "          dependencies within the model's input sequence length (typically 512 tokens for BERT).\n",
    "        - **Limitations:** Computationally more expensive. Has a fixed maximum input sequence length.\n",
    "          Interpretability of embeddings can be challenging. The quality of word-specific embeddings\n",
    "          can also depend on the subword tokenization and averaging strategy.\n",
    "        - **Comparison:** A significant leap from static and basic sequential embeddings. Provides much\n",
    "          richer, context-aware representations, forming the foundation for many modern NLP systems.\n",
    "        \"\"\"\n",
    "        plt.figtext(0.5, -0.08, summary_text, ha=\"center\", fontsize=10, bbox={\"facecolor\":\"lightcoral\", \"alpha\":0.2, \"pad\":5}, wrap=True)\n",
    "        plt.subplots_adjust(bottom=0.3) # Adjust layout\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough embeddings to perform PCA (need at least 2).\")\n",
    "else:\n",
    "    print(\"No specific word embeddings generated or too few to plot. Skipping PCA visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17297c23",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "\n",
    "In this notebook, we used a pre-trained BERT model to generate contextual embeddings.\n",
    "\n",
    "- **Key Observation:** The core demonstration was showing that the vector representation for a word like \"rabbit\" changes based on its surrounding sentence context. This is evident if `sim_rabbits` (and `sim_watches`) is less than 1.0. This dynamism is what makes BERT and other Transformer-based models so powerful.\n",
    "- **PCA Plot Insights:** The PCA plot visualizes these contextual instances. Words in very different contexts should appear further apart. Words used in similar contextual roles might cluster closer, even if they are different words. The plot now shows specific instances like \"rabbit\\_white\" and \"rabbit\\_with\\_watch\" as separate points.\n",
    "- **Beyond Word Embeddings:** BERT's `[CLS]` token embedding (the first token's output) is often used as an aggregate representation for the entire input sequence, especially for classification tasks. We used mean pooling here for sentence embeddings, another common technique.\n",
    "\n",
    "**Next Steps:** BERT opened the door to a wide array of Transformer models. Further stages in EmbedEvolution could explore:\n",
    "- **Sentence-BERT (Sentence Transformers):** Fine-tuning BERT (or other Transformers) specifically to create semantically meaningful sentence embeddings that can be easily compared using cosine similarity.\n",
    "- **More recent models:** Exploring variants like RoBERTa, ELECTRA, or domain-specific BERTs.\n",
    "- **Instruction-tuned models (like InstructOR):** Models trained to follow instructions, which can be powerful for generating tailored embeddings based on task descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
