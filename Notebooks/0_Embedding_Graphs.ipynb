{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ca389a",
   "metadata": {},
   "source": [
    "# Exploring Static Word Embeddings with FastText\n",
    "\n",
    "This script demonstrates key concepts of static word embeddings using a small pre-trained FastText model (wiki.simple, ~600 MB, 300-dimensional vectors). We focus on the word 'mercury', which has two meanings (a metal and a planet), to explore how embeddings capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory for saving plots\n",
    "Path(\"plots\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b50de",
   "metadata": {},
   "source": [
    "## Section 1: Download and Load Pre-trained FastText Model\n",
    "\n",
    "We use the wiki.simple model, a small pre-trained FastText model trained on English Wikipedia.\n",
    "This model is ~600 MB (compressed) and uses 300-dimensional vectors, making it suitable for educational purposes. We download the .bin.gz file from FastText's official repository and load it using Gensim's load_facebook_model function, which supports FastText's binary format.\n",
    "The model captures subword information, allowing it to handle out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74926f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\"\n",
    "model_zip_path = Path(\"models/wiki-news-300d-1M-subword.vec.zip\")\n",
    "model_vec_path = Path(\"models/wiki-news-300d-1M.vec\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "# Download and unzip the model only if the .vec file is not already present\n",
    "if not model_vec_path.exists():\n",
    "    print(\"Downloading wiki-news-300d-1M-subword FastText model (~600 MB)...\")\n",
    "    urllib.request.urlretrieve(model_url, model_zip_path)\n",
    "    print(f\"Model downloaded to {model_zip_path}\")\n",
    "    print(\"Unzipping model...\")\n",
    "    with zipfile.ZipFile(model_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(Path(\"models\"))\n",
    "    os.remove(model_zip_path)  # Clean up .zip file\n",
    "    print(f\"Model saved to {model_vec_path}\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading FastText model...\")\n",
    "model = KeyedVectors.load_word2vec_format(str(model_vec_path), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f4ea1",
   "metadata": {},
   "source": [
    "## t-SNE Visualization of the Enormous Map of Relations\n",
    "\n",
    "This section visualizes the enormous 'map of relations' in the FastText model, which contains 1 million words, each represented by a 300-dimensional vector (300 million total values).\n",
    "To emphasize the vastness and complexity, we sample 10,000 words (1% of the vocabulary) and create a deliberately dense scatter plot using t-SNE to reduce the 300-dimensional vectors to 2D.\n",
    "Only the core words (mercury, metals, planets) are highlighted and labeled to anchor the visualization, while the remaining points are plotted without labels to show the overwhelming density of relationships.\n",
    "This illustrates the massive scale of the embedding space, where words are positioned based on semantic relationships, but the sheer number of points makes individual relations hard to discern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eefa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_space_of_meaning(core_words, sample_size=10000):\n",
    "    # Sample additional random words from the model's vocabulary\n",
    "    vocab = list(model.key_to_index.keys())\n",
    "    random_words = random.sample(vocab, min(sample_size - len(core_words), len(vocab)))\n",
    "    sampled_words = core_words + random_words\n",
    "    print(f\"Sampled {len(sampled_words)} words for 3D t-SNE visualization\")\n",
    "    \n",
    "    # Get word vectors\n",
    "    vectors = np.array([model[word] for word in sampled_words])\n",
    "    \n",
    "    # Apply t-SNE for 3D visualization\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)\n",
    "    vectors_3d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = plt3D = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot all sampled words in gray with low alpha for dense, cluttered effect\n",
    "    ax.scatter(\n",
    "        vectors_3d[:, 0], vectors_3d[:, 1], vectors_3d[:, 2],\n",
    "        c='blue', alpha=0.1, s=5\n",
    "    )\n",
    "    \n",
    "    # Highlight core words in red\n",
    "    core_indices = range(len(core_words))\n",
    "    ax.scatter(\n",
    "        vectors_3d[core_indices, 0], vectors_3d[core_indices, 1], vectors_3d[core_indices, 2],\n",
    "        c='red', s=50, label='Core Words (Mercury, Metals, Planets)'\n",
    "    )\n",
    "    \n",
    "    # Label only core words\n",
    "    for i, word in enumerate(core_words):\n",
    "        ax.text(\n",
    "            vectors_3d[i, 0], vectors_3d[i, 1], vectors_3d[i, 2],\n",
    "            word, fontsize=12, color='black'\n",
    "        )\n",
    "    \n",
    "    ax.set_title(\"3D t-SNE Visualization of the Enormous Map of Relations\\n(1M Words Ã— 300 Dimensions, Sample of 10,000 Words)\")\n",
    "    ax.set_xlabel(\"t-SNE Dimension 1\")\n",
    "    ax.set_ylabel(\"t-SNE Dimension 2\")\n",
    "    ax.set_zlabel(\"t-SNE Dimension 3\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    output_path = Path(\"plots/tsne_map_of_relations_3d.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved 3D map of relations t-SNE plot to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870f47e",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49175ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    words = [\n",
    "        \"mercury\",\n",
    "        \"gold\",\n",
    "        \"venus\"\n",
    "    ]\n",
    "    # Generate t-SNE map of relations visualization\n",
    "    plot_tsne_space_of_meaning(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf4670",
   "metadata": {},
   "source": [
    "# Exploring Sequential Embeddings and Hidden States\n",
    "\n",
    "This script introduces sequential embeddings, focusing on the concept of the hidden state in Recurrent Neural Networks (RNNs) and their limitations. Unlike static embeddings (e.g., FastText), sequential embeddings process text sequentially, updating a hidden state to capture context.\n",
    "\n",
    "We use a simple RNN to process a sentence containing \"mercury\" and generate two plots:\n",
    "\n",
    "1. Hidden State Evolution Plot: Shows how the hidden state changes as the RNN processes each word, illustrating how it encodes temporal dependencies.\n",
    "2. Context Limitation Plot: Demonstrates the limitation of sequential embeddings by showing how cosine similarities between hidden states decrease for distant words, highlighting issues like vanishing gradients or limited context retention.\n",
    "\n",
    "Learning Objective: Understand the role of the hidden state in sequential models and recognize their limitations in capturing long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c80493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6dc8c7",
   "metadata": {},
   "source": [
    "## Section 1: Download and Load Pre-trained GloVe Embeddings\n",
    "\n",
    "We use a small pre-trained GloVe model (glove.6B.50d, ~160 MB) to embed words in our input sentence. GloVe provides static word embeddings, which we feed into an RNN to generate sequential embeddings. The hidden state of the RNN captures the context of the sequence, unlike static embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a74404",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "glove_zip_path = Path(\"models/glove.6B.zip\")\n",
    "glove_txt_path = Path(\"models/glove.6B.50d.txt\")\n",
    "\n",
    "# Create models directory\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "# Download and unzip GloVe if not already present\n",
    "if not glove_txt_path.exists():\n",
    "    if not glove_zip_path.exists():\n",
    "        print(\"Downloading GloVe 6B.50d model (~800 MB)...\")\n",
    "        urllib.request.urlretrieve(glove_url, glove_zip_path)\n",
    "        print(f\"Model downloaded to {glove_zip_path}\")\n",
    "    print(\"Unzipping model...\")\n",
    "    with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(Path(\"models\"))\n",
    "    os.remove(glove_zip_path)\n",
    "    print(f\"Model saved to {glove_txt_path}\")\n",
    "\n",
    "# Load GloVe embeddings into a dictionary\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word_to_vec = {}\n",
    "with open(glove_txt_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=np.float32)\n",
    "        word_to_vec[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668dcbf",
   "metadata": {},
   "source": [
    "## Section 2: Prepare Input Sentence\n",
    "\n",
    " We use a sample sentence containing \"mercury\" to demonstrate the hidden state. The sentence is tokenized, and each word is mapped to its GloVe embedding. Words not in GloVe are assigned a zero vector. This sentence ties into the theme of \"mercury\" as a metal and planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"mercury is a planet and a metal\".lower().split()\n",
    "vocab = list(set(sentence))  # Unique words\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "input_vectors = [word_to_vec.get(word, np.zeros(50)) for word in sentence]\n",
    "input_tensor = torch.tensor(input_vectors, dtype=torch.float32).unsqueeze(0)  # Shape: (1, seq_len, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eaaa8c",
   "metadata": {},
   "source": [
    "## Section 3: Define Simple RNN Model\n",
    "\n",
    "We define a basic RNN to process the sequence. The hidden state is a vector that updates at each timestep, encoding the context of the sequence up to that point. The hidden state is the key component of sequential embeddings, capturing temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "# Initialize model\n",
    "input_size = 50  # GloVe 50d\n",
    "hidden_size = 20  # Small hidden state for visualization\n",
    "output_size = len(vocab)  # Predict next word (toy task)\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "initial_hidden = torch.zeros(1, 1, hidden_size)  # Initial hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7867755",
   "metadata": {},
   "source": [
    "## Section 4: Hidden State Evolution Plot\n",
    "\n",
    "This section visualizes how the hidden state evolves as the RNN processes each word in the sentence. We reduce the hidden state (20 dimensions) to 2D using PCA for plotting. This shows how the hidden state captures the sequence's context, changing with each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden_state_evolution(sentence, input_tensor, model, initial_hidden):\n",
    "    model.eval()\n",
    "    hidden_states = []\n",
    "    \n",
    "    # Process sequence and collect hidden states\n",
    "    hidden = initial_hidden\n",
    "    with torch.no_grad():\n",
    "        for t in range(input_tensor.size(1)):\n",
    "            out, hidden = model(input_tensor[:, t:t+1, :], hidden)\n",
    "            hidden_states.append(hidden.squeeze().numpy())\n",
    "    \n",
    "    # Reduce hidden states to 2D using PCA\n",
    "    hidden_states = np.array(hidden_states)\n",
    "    pca = PCA(n_components=2)\n",
    "    hidden_2d = pca.fit_transform(hidden_states)\n",
    "    \n",
    "    # Plot hidden state trajectory\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(len(sentence)):\n",
    "        plt.scatter(hidden_2d[i, 0], hidden_2d[i, 1], s=100, label=sentence[i])\n",
    "        if i > 0:\n",
    "            plt.plot(hidden_2d[i-1:i+1, 0], hidden_2d[i-1:i+1, 1], 'b-')\n",
    "        plt.text(hidden_2d[i, 0] + 0.1, hidden_2d[i, 1], sentence[i], fontsize=12)\n",
    "    \n",
    "    plt.title(\"Hidden State Evolution Across Sequence\\n(RNN Processing 'mercury is a planet and a metal')\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    output_path = Path(\"plots/hidden_state_evolution.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved hidden state evolution plot to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418b58",
   "metadata": {},
   "source": [
    "## Section 5: Context Limitation Plot\n",
    "\n",
    "This section illustrates a key limitation of sequential embeddings: difficulty capturing long-range dependencies. We compute cosine similarities between the hidden state at each timestep and the final hidden state, showing how similarity decreases for earlier words, indicating loss of context (e.g., due to vanishing gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9716b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_context_limitation(sentence, input_tensor, model, initial_hidden):\n",
    "    model.eval()\n",
    "    hidden_states = []\n",
    "    \n",
    "    # Collect hidden states\n",
    "    hidden = initial_hidden\n",
    "    with torch.no_grad():\n",
    "        for t in range(input_tensor.size(1)):\n",
    "            out, hidden = model(input_tensor[:, t:t+1, :], hidden)\n",
    "            hidden_states.append(hidden.squeeze().numpy())\n",
    "    \n",
    "    hidden_states = np.array(hidden_states)\n",
    "    final_hidden = hidden_states[-1].reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarities with final hidden state\n",
    "    similarities = [cosine_similarity(hidden.reshape(1, -1), final_hidden)[0, 0] for hidden in hidden_states]\n",
    "    \n",
    "    # Plot similarities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(sentence)), similarities, marker='o')\n",
    "    for i, word in enumerate(sentence):\n",
    "        plt.text(i, similarities[i] + 0.02, word, fontsize=12)\n",
    "    plt.title(\"Context Limitation: Cosine Similarity to Final Hidden State\\n(Decreasing Similarity Shows Loss of Early Context)\")\n",
    "    plt.xlabel(\"Word Position in Sequence\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.tight_layout()\n",
    "    output_path = Path(\"plots/context_limitation.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved context limitation plot to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee8dfc",
   "metadata": {},
   "source": [
    "# Main Execution\n",
    "\n",
    "Run the visualizations to generate the plots, saved in the 'plots' directory. The GloVe model is downloaded and loaded if not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create plots directory\n",
    "    Path(\"plots\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate hidden state evolution plot\n",
    "    plot_hidden_state_evolution(sentence, input_tensor, model, initial_hidden)\n",
    "    \n",
    "    # Generate context limitation plot\n",
    "    plot_context_limitation(sentence, input_tensor, model, initial_hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
