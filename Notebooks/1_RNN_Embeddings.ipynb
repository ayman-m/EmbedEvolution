{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70ec356",
   "metadata": {},
   "source": [
    "# EmbedEvolution Stage 1: RNN Embeddings\n",
    "\n",
    "Welcome to the first stage of EmbedEvolution! Here, we explore how early sequence models like Recurrent Neural Networks (RNNs) began to capture context in language, going beyond static word vectors. We'll train a simple RNN, extract contextual representations, measure semantic similarity, and visualize the results.\n",
    "\n",
    "**Goal:** Understand how RNNs process sequences and generate context-dependent embeddings, observing their strengths and weaknesses (especially with long-range dependencies).\n",
    "\n",
    "%%\n",
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e220c",
   "metadata": {},
   "source": [
    "## 2. Define the Corpus\n",
    "We'll use a sample text to train our RNN and extract embeddings. A slightly longer and also fictional text to help illustrate context better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b10e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Lumina Codex and the Solara System: A Tapestry of Ancient Wisdom and Cosmic Discovery\n",
    "In the shadowed halls of the Cairo Museum, a dusty papyrus scroll, cataloged as Papyrus K-37b from the Middle Kingdom, lay forgotten for centuries. Dubbed the Lumina Codex by its discoverers, this fragile relic was initially dismissed as a mythological curiosity, its cryptic hieroglyphs and star charts interpreted as poetic musings of a priestly scribe. Yet, in 2024, a team of linguists and astronomers, led by Dr. Amara Nassar, deciphered its veiled verses, revealing an astonishing truth: the codex described a distant star system with uncanny precision, orbiting a radiant G-type star named the Star of Eternal Radiance—now known as Lumina. Intriguingly, the codex also spoke of a divine figure, \"the Swift One,\" bearing a staff entwined with serpents, reminiscent of the god Mercury in Roman mythology or Thoth in Egyptian lore. This messenger, often depicted as Thoth the scribe of the gods, is said to have imparted the knowledge of the stars to the ancient scribes, guiding their hands in creating the star charts. This revelation sparked a scientific odyssey, merging ancient Egyptian cosmology with cutting-edge astronomy, as the Solara System emerged from the Nebula Cygnus-X1, nestled in the Orion Spur of the Milky Way Galaxy.\n",
    "\n",
    "The Lumina Codex spoke of Lumina and its ten celestial attendants, organized into poetic regions: the searing Forges of Ra for the inner worlds, the verdant Blessed Belt of Osiris for the habitable zone, the majestic Domains of the Sky Titans for gas giants, and the enigmatic Frozen Outlands for the outer realms. Its star charts, etched with meticulous care, hinted at a cosmic map, with references to the Rivers of Stars—likely the Milky Way—and the Celestial Gardens, evoking the Local Group within the Virgo Supercluster. The codex's verses, such as \"Ten jewels dance in the embrace of the Eternal Radiance, their faces veiled in fire, water, and ice,\" seemed to prefigure a system now confirmed by the Cygnus-X1 Deep Sky Array, a fictional next-generation telescope orbiting beyond Earth's atmosphere.\n",
    "\n",
    "Discovery and Modern Corroboration\n",
    "The Solara System's discovery began in 2023, when the Cygnus-X1 Deep Sky Array detected subtle wobbles in Lumina's light, indicating a complex system of orbiting bodies. Located 1,200 light-years away in the Nebula Cygnus-X1, Lumina is a stable, middle-aged G-type star, slightly larger than the Sun, with a luminosity that sustains a diverse array of worlds. As astronomers analyzed the data, they identified ten planets, each with unique characteristics that eerily echoed the Lumina Codex. The parallels were undeniable: the codex's Forges of Ra matched the inner rocky planets, while the Blessed Belt of Osiris aligned with two habitable worlds teeming with life. The Domains of the Sky Titans and Frozen Outlands described gas giants and icy dwarfs with striking accuracy. The scientific community buzzed with excitement, as linguists and astronomers collaborated to decode the codex's metaphors, revealing a blend of ancient intuition and cosmic truth.\n",
    "\n",
    "The Solara System: A Celestial Menagerie\n",
    "Lumina: The Star of Eternal Radiance\n",
    "\n",
    "Lumina, a G2V star, radiates a warm, golden light, its stable fusion cycle supporting a system spanning 12 astronomical units. Its magnetic fields are unusually calm, suggesting a long lifespan conducive to life's evolution. The codex describes Lumina as \"the hearth of eternity, whose breath kindles the dance of worlds,\" a poetic nod to its life-giving energy.\n",
    "\n",
    "The Forges of Ra: Inner Planets\n",
    "1- Ignis: The closest planet to Lumina, Ignis is a scorched, iron-rich world with a molten surface pocked by ancient impact craters, reminiscent of the planet Mercury in our own solar system. Its thin atmosphere, rich in sulfur dioxide, glows faintly under Lumina's intense radiation. The codex calls it \"Ra's Anvil, where molten rivers forge the bones of the cosmos,\" reflecting its volcanic past and metallic crust.\n",
    "2- Ferrus: Slightly larger, Ferrus is a rocky planet with vast plains of oxidized iron, giving it a crimson hue. Its surface bears scars of past tectonic activity, with towering cliffs and deep chasms. The codex names it \"the Forge of Hephaestus's Twin,\" hinting at its metallic wealth, now confirmed by spectroscopic analysis revealing nickel and cobalt deposits.\n",
    "\n",
    "The Blessed Belt of Osiris: Habitable Zone\n",
    "1- Aqua: A breathtaking ocean world, Aqua is enveloped in turquoise clouds of water vapor and nitrogen. Its surface is 90% liquid water, with archipelagos of coral-like structures hosting complex aquatic ecosystems. Bioluminescent Aquarelles, jellyfish-like creatures with crystalline tentacles, drift in vast schools, their light pulses synchronizing in rhythmic displays. Predatory Thalacynths, eel-like organisms with electromagnetic sensors, hunt in the deep trenches. Aqua's moon, Thalassa, is an ice-covered world with a subglacial ocean, where astrobiologists hypothesize microbial extremophiles thrive in hydrothermal vents, metabolizing sulfur compounds. The codex describes Aqua as \"Osiris's Chalice, where life swims in the tears of the gods,\" and Thalassa as \"the frozen veil hiding the spark of creation.\"\n",
    "2- Veridia: A super-Earth, Veridia boasts lush continents of bioluminescent flora, such as Luminara trees, which pulse with green and violet light, and Crystalferns, whose fractal leaves refract Lumina's rays into dazzling spectra. Veridia is home to the Sylvans, sentient, silicon-based life forms resembling ambulatory crystal shrubs. Their bodies, composed of lattice-like structures, shimmer with bioluminescent patterns used for communication. Intriguingly, the Sylvans' technology incorporates liquid mercury, a metal known for its unique properties, in their communication devices. This allows them to transmit their bioluminescent patterns through conductive channels, enhancing their collective consciousness. Sylvan society is decentralized, with \"groves\" of individuals linked via light-based signals, forming a collective consciousness deeply attuned to Veridia's ecosystem. Their architecture, grown from crystalline minerals, integrates seamlessly with the landscape. The codex calls Veridia \"the Garden of Osiris's Breath,\" where \"the shining ones weave light into wisdom.\"\n",
    "\n",
    "The Domains of the Sky Titans: Gas Giants\n",
    "1- Zephyrus: A massive hydrogen-helium gas giant, Zephyrus dominates the system with its radiant ring system, composed of ice and silicate particles. Its atmosphere swirls with golden storms, driven by intense winds. Among its 47 moons, Io-Prime stands out, a volcanically active world spewing sulfur plumes, likely powered by tidal heating. The codex names Zephyrus \"the Sky Titan's Crown,\" its rings \"the jeweled girdle of the heavens.\"\n",
    "2- Boreas: An ice giant with a deep blue methane atmosphere, Boreas exhibits retrograde rotation and an asymmetrical magnetic field, creating auroras that dance across its poles. Its 22 moons include Erynnis, a rocky moon with methane lakes. The codex describes Boreas as \"the Frost Titan, whose breath chills the void,\" capturing its icy majesty.\n",
    "\n",
    "The Frozen Outlands: Outer Planets\n",
    "1- Umbriel: A dwarf planet with a charcoal-dark surface, Umbriel's icy crust is fractured by ancient impacts. Its moon Nyx, a captured object, is rich in organic compounds, hinting at prebiotic chemistry. The codex calls Umbriel \"the Shadowed Outcast, guarded by the dark sentinel.\"\n",
    "2- Erebus: An icy world with a nitrogen-methane atmosphere, Erebus has a highly elliptical orbit, suggesting a captured origin. Its surface sparkles with frost-covered ridges. The codex names it \"the Silent Wanderer, cloaked in eternal frost.\"\n",
    "3- Aetheria: The outermost planet, Aetheria is a rogue dwarf with a thin atmosphere of neon and argon. Its moon Lethe exhibits cryovolcanism, spewing ammonia-water mixtures. Astrobiologists speculate that Lethe's subsurface ocean may harbor microbial life, analogous to Thalassa's. The codex describes Aetheria as \"the Veiled Wanderer, whose dreams freeze in the outer dark,\" and Lethe as \"the weeping mirror of the cosmos.\"\n",
    "4- Nyxara: A small, icy body with a chaotic orbit, Nyxara's surface is a mosaic of frozen nitrogen and carbon monoxide. The codex calls it \"the Lost Jewel, dancing beyond the Titans' gaze.\"\n",
    "\n",
    "Life in the Solara System\n",
    "Aqua's aquatic ecosystems are a marvel, with Aquarelles forming symbiotic networks with coral-like Hydroskeletons, which filter nutrients from the water. Thalacynths use electromagnetic pulses to stun prey, suggesting an evolutionary arms race. On Thalassa, microbial life is hypothesized based on chemical signatures of sulfur and methane in its subglacial ocean, though no direct evidence exists yet.\n",
    "Veridia's Sylvans are the system's crown jewel. Their crystalline bodies, averaging two meters tall, refract light into complex patterns, encoding emotions, ideas, and memories. Their society operates as a \"luminous collective,\" with no central authority; decisions emerge from synchronized light displays across groves. Sylvan technology manipulates crystalline minerals and liquid mercury to create tools and habitats, all in harmony with Veridia's ecosystem. Their discovery has sparked intense study by linguists decoding their light-based language, revealing a philosophy centered on balance and interconnectedness.\n",
    "On Lethe, cryovolcanic activity suggests a subsurface ocean with potential microbial ecosystems, possibly metabolizing ammonia. Unlike Aqua's confirmed complex life and Veridia's sentient Sylvans, life on Thalassa and Lethe remains speculative, driving astrobiological research.\n",
    "\n",
    "Galactic Context\n",
    "The Solara System resides in the Orion Spur, a minor arm of the Milky Way, part of the Local Group within the Virgo Supercluster. The codex's Rivers of Stars evoke the Milky Way's spiral arms, while the Celestial Gardens suggest a poetic grasp of the Local Group's galactic cluster. This cosmic placement underscores Solara's significance as a microcosm of the universe's diversity.\n",
    "\n",
    "Ongoing Exploration\n",
    "Scientific teams, including astrobiologists, geologists, and linguists, are studying Solara via the Cygnus-X1 Deep Sky Array and planned probes, such as the Lumina Pathfinder Mission. Challenges include the 1,200-light-year distance, requiring advanced telemetry for data transmission. Sylvan communication poses a unique hurdle, as their light patterns defy traditional linguistic models. Future missions aim to deploy orbiters around Aqua, Veridia, and Lethe to confirm microbial life and study Sylvan culture.\n",
    "\n",
    "A Cosmic Tapestry\n",
    "The Solara System, unveiled through the Lumina Codex and modern astronomy, blends ancient wisdom with scientific discovery. Its worlds—from the fiery Forges of Ra to the icy Frozen Outlands—offer a rich tapestry of environments, life forms, and mysteries. As scientists probe this distant system, the codex's poetic verses resonate, reminding humanity that the cosmos has long whispered its secrets, awaiting those bold enough to listen.\n",
    "\n",
    "To share this cosmic tapestry with the world, an educational initiative has launched the Solara Explorer Kit for children. The kit includes detailed models of the ten planets, with Ignis crafted to resemble the planet Mercury as a toy for the kids to play and learn, it's complete with its cratered surface and a tiny, movable rover that kids can roll across its terrain. A central figure of Lumina, the star, shines brightly, paired with a glow-in-the-dark Mercury space blaster that shoots safe, squishy foam darts, letting kids pretend they're defending the Solara System from space invaders. This kit aims to spark curiosity in young minds, inviting them to explore the night sky and imagine distant worlds through hands-on fun.\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text slightly\n",
    "\n",
    "text = text.lower()\n",
    "text = re.sub(r'[^a-z\\s]', '', text) # Keep only lowercase letters and spaces\n",
    "text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "\n",
    "print(\"Cleaned Text Sample:\")\n",
    "print(text[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcebc5",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "We need to convert the text into numerical sequences that the RNN can process.\n",
    "1.  **Tokenization:** Split the text into words (tokens).\n",
    "2.  **Vocabulary Creation:** Build a mapping from unique words to integer indices.\n",
    "3.  **Sequence Generation:** Create input sequences (e.g., sequences of 5 words) and corresponding target words (the 6th word) for training the RNN to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<unk>\") # Add OOV token for any novel terms during generation\n",
    "tokenizer.fit_on_texts([text])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "tokens = text.split(' ')\n",
    "seq_length = 5 # Length of input sequence for prediction, can be tuned\n",
    "sequences = []\n",
    "\n",
    "for i in range(seq_length, len(tokens)):\n",
    "    input_seq_tokens = tokens[i-seq_length:i]\n",
    "    target_word_token = tokens[i]\n",
    "    sequences.append((input_seq_tokens, target_word_token))\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "if sequences:\n",
    "    print(\"Sample sequence input:\", sequences[0][0])\n",
    "    print(\"Sample sequence output:\", sequences[0][1])\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "for input_tokens, target_token in sequences:\n",
    "    # Convert list of token strings to list of indices\n",
    "    current_X_indices = [word_index.get(token, word_index[\"<unk>\"]) for token in input_tokens]\n",
    "    X_list.append(current_X_indices)\n",
    "    y_list.append(word_index.get(target_token, word_index[\"<unk>\"]))\n",
    "\n",
    "X = pad_sequences(X_list, maxlen=seq_length, padding='pre')\n",
    "y = np.array(y_list)\n",
    "\n",
    "if X.size > 0:\n",
    "    print(\"\\nSample encoded X (after padding):\", X[0])\n",
    "    print(\"Sample encoded y:\", y[0])\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15437f",
   "metadata": {},
   "source": [
    "## 4. Build the Simple RNN Model\n",
    "We'll create a sequential model with:\n",
    "1.  An `Embedding` layer: Maps word indices to dense vectors.\n",
    "2.  A `SimpleRNN` layer: Processes the sequence of embeddings.\n",
    "3.  A `Dense` layer: Outputs predictions for the next word over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # Dimensionality of the learned word vectors\n",
    "rnn_units = 128   # Number of units in the SimpleRNN layer (memory capacity)\n",
    "\n",
    "# Ensure X is not empty before proceeding\n",
    "if X.shape[0] == 0:\n",
    "    print(\"Error: No training sequences generated. Check text preprocessing and corpus size.\")\n",
    "    model = None # Or handle error appropriately\n",
    "else:\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length),\n",
    "        SimpleRNN(units=rnn_units),\n",
    "        Dense(units=vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.build(input_shape=(None, seq_length))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad34f4b",
   "metadata": {},
   "source": [
    "## 5. Train the RNN Model\n",
    "\n",
    "We'll train the model for a small number of epochs. The goal isn't perfect prediction, but rather to get the Embedding and RNN layers to learn some representation of the sequence context.\n",
    "*Note: Training RNNs on larger texts can be time-consuming. We use a small dataset and few epochs for demonstration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # Adjust as needed for better results vs. training time\n",
    "batch_size = 64\n",
    "print(\"\\nTraining RNN model...\")\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f724965",
   "metadata": {},
   "source": [
    "## 6. Analyzing Contextual Keyword Embeddings: Similarity Heatmap & PCA Visualization\n",
    "\n",
    "We'll extract the RNN's hidden state after processing short sequences ending with (or containing) our target keywords from the \"Lumina Codex.\" This hidden state acts as a contextual embedding for that keyword in its specific preceding context. We will then:\n",
    "1.  Visualize the cosine similarities between these contextual keyword embeddings using a heatmap.\n",
    "2.  Visualize the embeddings themselves in 2D using PCA to observe potential clustering.\n",
    "This helps evaluate Semantic Similarity, Contextual Understanding, and Clustering Quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_to_analyze = [\"mercury\", \"lumina\", \"star\", \"sun\", \"ignis\", \"ferrus\", \"iron\", \"thoth\", \"orbit\", \"nebula\"]\n",
    "\n",
    "# Ensure prerequisite variables from previous cells are available:\n",
    "# model, tokenizer, word_index, tokens, seq_length, rnn_units\n",
    "\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Model not trained. Please run model training cell first.\")\n",
    "elif 'tokenizer' not in locals() or 'word_index' not in locals():\n",
    "    print(\"Tokenizer or word_index not available. Please run text preprocessing cell.\")\n",
    "else:\n",
    "    # Debug: Check which keywords are in the vocabulary\n",
    "    keywords_in_vocab = [kw for kw in keywords_to_analyze if kw in word_index]\n",
    "    if not keywords_in_vocab:\n",
    "        print(\"Warning: None of the specified keywords_to_analyze are in the trained vocabulary.\")\n",
    "    else:\n",
    "        print(f\"Keywords from list found in vocabulary for analysis ({len(keywords_in_vocab)} words): {keywords_in_vocab}\")\n",
    "\n",
    "        # Function to find short sequences in the text containing a keyword\n",
    "        def get_sequences_for_keywords(text_tokens, keywords, seq_len, max_sequences_per_keyword=1):\n",
    "            keyword_sequences = {}\n",
    "            for kw in keywords:\n",
    "                kw_sequences_found = []\n",
    "                # Case-insensitive search\n",
    "                for i in range(len(text_tokens) - seq_len + 1):\n",
    "                    window = text_tokens[i : i + seq_len]\n",
    "                    # Check if keyword is in window (case-insensitive)\n",
    "                    if any(token.lower() == kw.lower() for token in window):\n",
    "                        kw_sequences_found.append(window)\n",
    "                        if max_sequences_per_keyword and len(kw_sequences_found) >= max_sequences_per_keyword:\n",
    "                            break\n",
    "                if kw_sequences_found:\n",
    "                    if max_sequences_per_keyword == 1:\n",
    "                        label = kw\n",
    "                        if label not in keyword_sequences:\n",
    "                            keyword_sequences[label] = kw_sequences_found[0]\n",
    "                    else:\n",
    "                        for idx, seq in enumerate(kw_sequences_found):\n",
    "                            label = f\"{kw}_{idx+1}\"\n",
    "                            if label not in keyword_sequences:\n",
    "                                keyword_sequences[label] = seq\n",
    "            return keyword_sequences\n",
    "\n",
    "        embedding_layer = model.layers[0]\n",
    "        rnn_layer = model.layers[1]\n",
    "        rnn_output_model = Sequential([embedding_layer, rnn_layer])\n",
    "        rnn_output_model.build(input_shape=(None, seq_length))\n",
    "\n",
    "        def get_rnn_contextual_embedding(token_list):\n",
    "            token_list_str = [str(t) for t in token_list]\n",
    "            encoded_sequence = [word_index.get(token, word_index.get(\"<unk>\", 0)) for token in token_list_str]\n",
    "            padded_sequence = pad_sequences([encoded_sequence], maxlen=seq_length, padding='pre')\n",
    "            if padded_sequence.shape[1] == 0:\n",
    "                return np.zeros(rnn_units)\n",
    "            return rnn_output_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "        # === Section 1: General Keyword Analysis ===\n",
    "        print(\"\\n=== Section 1: General Keyword Analysis ===\")\n",
    "        contextual_phrase_dict = get_sequences_for_keywords(tokens, keywords_in_vocab, seq_length, max_sequences_per_keyword=1)\n",
    "\n",
    "        embeddings_for_analysis = {}\n",
    "        plot_labels_for_analysis = []\n",
    "\n",
    "        print(\"\\nExtracting RNN hidden states for keyword contexts:\")\n",
    "        for name, phrase_tokens in contextual_phrase_dict.items():\n",
    "            if not phrase_tokens:\n",
    "                print(f\"Warning: No tokens for '{name}'. Skipping.\")\n",
    "                continue\n",
    "            embedding = get_rnn_contextual_embedding(phrase_tokens)\n",
    "            embeddings_for_analysis[name] = embedding\n",
    "            plot_labels_for_analysis.append(name)\n",
    "            print(f\"- Extracted embedding for '{name}' using context: '{' '.join(phrase_tokens)}'\")\n",
    "\n",
    "        if embeddings_for_analysis and len(embeddings_for_analysis) >= 2:\n",
    "            embedding_matrix_analysis = np.array(list(embeddings_for_analysis.values()))\n",
    "            print(\"\\nShape of embedding matrix for analysis:\", embedding_matrix_analysis.shape)\n",
    "\n",
    "            # --- 1. Semantic Similarity & 2. Contextual Understanding (via Heatmap) ---\n",
    "            print(\"\\n--- Cosine Similarity Heatmap (Contextual Keywords) ---\")\n",
    "            similarity_matrix_keywords = cosine_similarity(embedding_matrix_analysis)\n",
    "            \n",
    "            num_labels = len(plot_labels_for_analysis)\n",
    "            fig_width = max(12, num_labels * 0.7)\n",
    "            fig_height = max(10, num_labels * 0.5)\n",
    "            plt.figure(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            annotate_heatmap = num_labels < 25\n",
    "\n",
    "            # Enhanced color map with better contrast\n",
    "            sns.heatmap(similarity_matrix_keywords,\n",
    "                        annot=annotate_heatmap,\n",
    "                        cmap='viridis', \n",
    "                        fmt=\".2f\",\n",
    "                        xticklabels=plot_labels_for_analysis,\n",
    "                        yticklabels=plot_labels_for_analysis,\n",
    "                        linewidths=.5,\n",
    "                        cbar_kws={\"shrink\": .8},\n",
    "                        vmin=0,  # Set minimum value for better contrast\n",
    "                        vmax=1)  # Set maximum value for better contrast\n",
    "            plt.title(f'RNN Cosine Similarity (Contextual Keywords - Lumina Codex)', fontsize=16)\n",
    "            plt.xticks(rotation=65, ha='right', fontsize=max(8, 14 - num_labels // 4))\n",
    "            plt.yticks(rotation=0, fontsize=max(8, 14 - num_labels // 4))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Print a few specific similarities to discuss\n",
    "            print(\"\\n--- Specific Pairwise Similarities for Discussion ---\")\n",
    "            pairs_to_check_for_discussion = [\n",
    "                (\"mercury\", \"ignis\"), (\"lumina\", \"star\"), (\"star\", \"sun\"),\n",
    "                (\"ignis\", \"ferrus\"), (\"mercury\", \"iron\"), (\"thoth\", \"mercury\"),\n",
    "                (\"orbit\", \"lumina\"), (\"nebula\", \"lumina\")\n",
    "            ]\n",
    "            for kw1_label, kw2_label in pairs_to_check_for_discussion:\n",
    "                if kw1_label in embeddings_for_analysis and kw2_label in embeddings_for_analysis:\n",
    "                    sim_val = cosine_similarity(\n",
    "                        embeddings_for_analysis[kw1_label].reshape(1,-1),\n",
    "                        embeddings_for_analysis[kw2_label].reshape(1,-1)\n",
    "                    )[0][0]\n",
    "                    print(f\"Similarity between '{kw1_label}' and '{kw2_label}': {sim_val:.4f}\")\n",
    "\n",
    "            # --- 4. Clustering Quality (PCA Visualization) ---\n",
    "            print(\"\\n--- PCA Visualization (Contextual Keywords) ---\")\n",
    "            pca = PCA(n_components=2)\n",
    "            embeddings_2d_pca = pca.fit_transform(embedding_matrix_analysis)\n",
    "\n",
    "            plt.figure(figsize=(fig_width * 0.9, fig_height * 0.9))\n",
    "            \n",
    "            # Define keyword categories and colors\n",
    "            keyword_categories = {\n",
    "                'celestial': ['star', 'sun', 'lumina', 'orbit', 'nebula'],\n",
    "                'elements': ['mercury', 'ignis', 'ferrus', 'iron'],\n",
    "                'mythology': ['thoth']\n",
    "            }\n",
    "            \n",
    "            # Create color mapping\n",
    "            colors = []\n",
    "            for label in plot_labels_for_analysis:\n",
    "                if label in keyword_categories['celestial']:\n",
    "                    colors.append('gold')\n",
    "                elif label in keyword_categories['elements']:\n",
    "                    colors.append('darkred')\n",
    "                elif label in keyword_categories['mythology']:\n",
    "                    colors.append('purple')\n",
    "                else:\n",
    "                    colors.append('gray')\n",
    "            \n",
    "            # Plot with colors\n",
    "            plt.scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], \n",
    "                       alpha=0.8, s=100, c=colors, edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            # Add annotations with better positioning\n",
    "            for i, label in enumerate(plot_labels_for_analysis):\n",
    "                plt.annotate(label, \n",
    "                           (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]), \n",
    "                           textcoords=\"offset points\", \n",
    "                           xytext=(5,5), \n",
    "                           ha='center', \n",
    "                           fontsize=max(8, 12 - num_labels // 5),\n",
    "                           fontweight='bold')\n",
    "            \n",
    "            plt.title('RNN Contextual Keyword Embeddings (Lumina Codex) - PCA', fontsize=16)\n",
    "            plt.xlabel('PCA Component 1', fontsize=12)\n",
    "            plt.ylabel('PCA Component 2', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add legend for categories\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='gold', edgecolor='black', label='Celestial'),\n",
    "                Patch(facecolor='darkred', edgecolor='black', label='Elements'),\n",
    "                Patch(facecolor='purple', edgecolor='black', label='Mythology')\n",
    "            ]\n",
    "            plt.legend(handles=legend_elements, \n",
    "                      loc='upper center', \n",
    "                      bbox_to_anchor=(0.5, -0.1), \n",
    "                      ncol=3,\n",
    "                      fontsize=10,\n",
    "                      title='Keyword Categories')\n",
    "            \n",
    "            # Adjust layout to prevent legend cutoff\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(bottom=0.15)\n",
    "            plt.show()\n",
    "            \n",
    "            # Print explained variance ratio\n",
    "            print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "        else:\n",
    "            print(\"Not enough keyword embeddings (need at least 2) for similarity matrix or PCA plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec4169",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis of 'mercury' Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd32b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to analyze - expanded to include toy, metal, mythology\n",
    "keywords_to_analyze = [\"mercury\", \"planet\", \"toy\", \"metal\", \"mythology\", \"lumina\", \"star\", \"sun\", \"ignis\", \"ferrus\", \"iron\", \"thoth\", \"orbit\", \"nebula\"]\n",
    "\n",
    "# Ensure prerequisite variables from previous cells are available:\n",
    "# model, tokenizer, word_index, tokens, seq_length, rnn_units\n",
    "\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Model not trained. Please run model training cell first.\")\n",
    "elif 'tokenizer' not in locals() or 'word_index' not in locals():\n",
    "    print(\"Tokenizer or word_index not available. Please run text preprocessing cell.\")\n",
    "else:\n",
    "    # Debug: Check which keywords are in the vocabulary\n",
    "    keywords_in_vocab = [kw for kw in keywords_to_analyze if kw in word_index]\n",
    "    if not keywords_in_vocab:\n",
    "        print(\"Warning: None of the specified keywords_to_analyze are in the trained vocabulary.\")\n",
    "    else:\n",
    "        print(f\"Keywords from list found in vocabulary for analysis ({len(keywords_in_vocab)} words): {keywords_in_vocab}\")\n",
    "\n",
    "        # Debug: Check for all \"mercury\" occurrences in tokens\n",
    "        mercury_positions = [i for i, token in enumerate(tokens) if token.lower() == \"mercury\"]\n",
    "        print(f\"\\nDiagnostic: Found {len(mercury_positions)} occurrences of 'mercury' in tokens at positions: {mercury_positions}\")\n",
    "\n",
    "        # Updated function to find sequences for each unique occurrence of a keyword\n",
    "        def get_sequences_for_keywords(text_tokens, keywords, seq_len, max_sequences_per_keyword=None):\n",
    "            keyword_sequences = {}\n",
    "            for kw in keywords:\n",
    "                # Find all positions of the keyword (case-insensitive)\n",
    "                kw_positions = [i for i, token in enumerate(text_tokens) if token.lower() == kw]\n",
    "                kw_sequences_found = []\n",
    "                used_positions = set()  # Track positions to avoid overlap\n",
    "\n",
    "                for pos in kw_positions:\n",
    "                    # Find the earliest window that includes this occurrence\n",
    "                    start = max(0, pos - seq_len + 1)\n",
    "                    end = min(len(text_tokens), pos + seq_len)\n",
    "                    for i in range(start, pos + 1):\n",
    "                        window = text_tokens[i:i + seq_len]\n",
    "                        if len(window) == seq_len and kw.lower() in [w.lower() for w in window] and i not in used_positions:\n",
    "                            kw_sequences_found.append(window)\n",
    "                            used_positions.update(range(i, i + seq_len))\n",
    "                            break  # Take the first valid window for this occurrence\n",
    "\n",
    "                    # Stop if we've reached the max number of sequences\n",
    "                    if max_sequences_per_keyword and len(kw_sequences_found) >= max_sequences_per_keyword:\n",
    "                        break\n",
    "\n",
    "                # Assign labels based on the number of sequences found\n",
    "                if kw_sequences_found:\n",
    "                    if max_sequences_per_keyword == 1 and len(kw_positions) <= 1:\n",
    "                        label = kw\n",
    "                        if label not in keyword_sequences:\n",
    "                            keyword_sequences[label] = kw_sequences_found[0]\n",
    "                    else:\n",
    "                        for idx, seq in enumerate(kw_sequences_found):\n",
    "                            label = f\"{kw}_{idx+1}\" if len(kw_positions) > 1 else kw\n",
    "                            if label not in keyword_sequences:\n",
    "                                keyword_sequences[label] = seq\n",
    "            return keyword_sequences\n",
    "\n",
    "        embedding_layer = model.layers[0]\n",
    "        rnn_layer = model.layers[1]\n",
    "        rnn_output_model = Sequential([embedding_layer, rnn_layer])\n",
    "        rnn_output_model.build(input_shape=(None, seq_length))\n",
    "\n",
    "        def get_rnn_contextual_embedding(token_list):\n",
    "            token_list_str = [str(t) for t in token_list]\n",
    "            encoded_sequence = [word_index.get(token, word_index.get(\"<unk>\", 0)) for token in token_list_str]\n",
    "            padded_sequence = pad_sequences([encoded_sequence], maxlen=seq_length, padding='pre')\n",
    "            if padded_sequence.shape[1] == 0:\n",
    "                return np.zeros(rnn_units)\n",
    "            return rnn_output_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "        # === Section 2: Analysis of All 'mercury' Occurrences and Related Keywords ===\n",
    "        print(\"\\n=== Section 2: Analysis of All 'mercury' Occurrences and Related Keywords ===\")\n",
    "        \n",
    "        # Keywords to extract contexts for\n",
    "        analysis_keywords = [\"mercury\", \"planet\", \"toy\", \"metal\", \"mythology\"]\n",
    "        \n",
    "        # Check if all keywords are in vocabulary\n",
    "        missing_keywords = [kw for kw in analysis_keywords if kw not in word_index]\n",
    "        if missing_keywords:\n",
    "            print(f\"Warning: The following keywords are not in the trained vocabulary: {missing_keywords}\")\n",
    "        \n",
    "        # Extract all contexts for 'mercury' without limiting to one sequence total\n",
    "        mercury_contexts_dict = get_sequences_for_keywords(tokens, [\"mercury\"], seq_length, max_sequences_per_keyword=None)\n",
    "        \n",
    "        # Extract contexts for other keywords (limit to 1 sequence each for simplicity)\n",
    "        other_contexts = {}\n",
    "        for kw in [\"planet\", \"toy\", \"metal\", \"mythology\"]:\n",
    "            if kw in word_index:\n",
    "                contexts = get_sequences_for_keywords(tokens, [kw], seq_length, max_sequences_per_keyword=1)\n",
    "                if contexts:\n",
    "                    other_contexts.update(contexts)\n",
    "\n",
    "        # Hardcoded list of mercury categories based on text analysis (6 occurrences)\n",
    "        mercury_categories = [\"mythology\", \"planet\", \"metal\", \"metal\", \"planet\", \"toy\"]\n",
    "\n",
    "        # Manually assign context-specific labels to 'mercury' occurrences based on the category list\n",
    "        all_embeddings = {}\n",
    "        all_labels = []\n",
    "        label_counts = {\"mythology\": 0, \"planet\": 0, \"metal\": 0, \"toy\": 0}  # Track indices for each category\n",
    "\n",
    "        print(\"\\nExtracting RNN hidden states for all 'mercury' contexts:\")\n",
    "        for idx, (label, phrase_tokens) in enumerate(mercury_contexts_dict.items()):\n",
    "            if not phrase_tokens:\n",
    "                print(f\"Warning: No tokens for '{label}'. Skipping.\")\n",
    "                continue\n",
    "            if idx >= len(mercury_categories):\n",
    "                print(f\"Warning: Extra occurrence detected at index {idx}, skipping to match 6 occurrences.\")\n",
    "                continue  # Skip extra occurrences to align with the 6-category list\n",
    "            category = mercury_categories[idx]\n",
    "            label_counts[category] += 1\n",
    "            context_label = f\"mercury_{category}_{label_counts[category]}\"\n",
    "            embedding = get_rnn_contextual_embedding(phrase_tokens)\n",
    "            all_embeddings[context_label] = embedding\n",
    "            all_labels.append(context_label)\n",
    "            print(f\"- Extracted embedding for '{context_label}' using context: '{' '.join(phrase_tokens)}'\")\n",
    "\n",
    "        # Extract embeddings for other keywords\n",
    "        print(\"\\nExtracting RNN hidden states for other keywords:\")\n",
    "        for label, phrase_tokens in other_contexts.items():\n",
    "            if not phrase_tokens:\n",
    "                print(f\"Warning: No tokens for '{label}'. Skipping.\")\n",
    "                continue\n",
    "            embedding = get_rnn_contextual_embedding(phrase_tokens)\n",
    "            # Use the keyword itself as the label\n",
    "            keyword_label = label.split('_')[0]  # Get the base keyword without any suffix\n",
    "            all_embeddings[keyword_label] = embedding\n",
    "            all_labels.append(keyword_label)\n",
    "            print(f\"- Extracted embedding for '{keyword_label}' using context: '{' '.join(phrase_tokens)}'\")\n",
    "\n",
    "        # Diagnostic: Check number of extracted embeddings\n",
    "        print(f\"\\nDiagnostic: Total embeddings extracted: {len(all_embeddings)}\")\n",
    "        print(f\"Labels: {all_labels}\")\n",
    "\n",
    "        # Proceed with analysis if there are at least 2 total embeddings\n",
    "        if len(all_embeddings) >= 2:\n",
    "            # Create embedding matrix from all embeddings\n",
    "            embedding_matrix = np.array(list(all_embeddings.values()))\n",
    "            print(\"\\nShape of embedding matrix for all contexts:\", embedding_matrix.shape)\n",
    "\n",
    "            # Heatmap for all contexts\n",
    "            print(\"\\n--- Cosine Similarity Heatmap (All Contexts) ---\")\n",
    "            similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "            \n",
    "            num_labels = len(all_labels)\n",
    "            fig_width = max(10, num_labels * 0.8)\n",
    "            fig_height = max(8, num_labels * 0.6)\n",
    "            plt.figure(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            annotate_heatmap = num_labels < 15\n",
    "            \n",
    "            sns.heatmap(similarity_matrix,\n",
    "                        annot=annotate_heatmap,\n",
    "                        cmap='viridis',\n",
    "                        fmt=\".2f\",\n",
    "                        xticklabels=all_labels,\n",
    "                        yticklabels=all_labels,\n",
    "                        linewidths=.5,\n",
    "                        cbar_kws={\"shrink\": .8})\n",
    "            plt.title(f'RNN Cosine Similarity (All Contexts - Lumina Codex)', fontsize=16)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.yticks(rotation=0, fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # PCA for all contexts\n",
    "            print(\"\\n--- PCA Visualization (All Contexts) ---\")\n",
    "            pca = PCA(n_components=2)\n",
    "            embeddings_2d_pca = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "            plt.figure(figsize=(fig_width * 0.9, fig_height * 0.9))\n",
    "            \n",
    "            # Create color map for different keyword types\n",
    "            colors = []\n",
    "            for label in all_labels:\n",
    "                if 'mercury' in label:\n",
    "                    colors.append('red')\n",
    "                elif label == 'planet':\n",
    "                    colors.append('blue')\n",
    "                elif label == 'toy':\n",
    "                    colors.append('green')\n",
    "                elif label == 'metal':\n",
    "                    colors.append('orange')\n",
    "                elif label == 'mythology':\n",
    "                    colors.append('purple')\n",
    "                else:\n",
    "                    colors.append('gray')\n",
    "            \n",
    "            plt.scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], alpha=0.7, s=100, c=colors)\n",
    "            \n",
    "            for i, label in enumerate(all_labels):\n",
    "                plt.annotate(label, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]), \n",
    "                            textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=10)\n",
    "            \n",
    "            plt.title('RNN Contextual Embeddings (All Contexts - Lumina Codex) - PCA', fontsize=16)\n",
    "            plt.xlabel('PCA Component 1', fontsize=12)\n",
    "            plt.ylabel('PCA Component 2', fontsize=12)\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Add legend for colors\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='red', label='mercury contexts'),\n",
    "                Patch(facecolor='blue', label='planet'),\n",
    "                Patch(facecolor='green', label='toy'),\n",
    "                Patch(facecolor='orange', label='metal'),\n",
    "                Patch(facecolor='purple', label='mythology')\n",
    "            ]\n",
    "            plt.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Not enough total embeddings (need at least 2) for similarity matrix or PCA plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e660780",
   "metadata": {},
   "source": [
    "## 8. Illustrating Limitations: A Code-Driven Look at RNN Embeddings with the \"Lumina Codex\"\n",
    "\n",
    "The conceptual limitations of Recurrent Neural Networks become more concrete when we examine their performance on our \"Lumina Codex\" corpus. In the `EmbedEvolution` RNN notebook (`1_RNN_Embeddings.ipynb`), we trained a SimpleRNN for next-word prediction and then evaluated its ability to understand context and generate text.\n",
    "\n",
    "* **Code Setup Snapshot:**\n",
    "    The experiment involved using **TensorFlow/Keras** to build a `Sequential` model comprising an `Embedding` layer, a `SimpleRNN` layer, and a `Dense` output layer. Text preprocessing utilized the Keras `Tokenizer`, while analysis involved **NumPy** for numerical tasks, **Scikit-learn** for `cosine_similarity` and `PCA`, and **Matplotlib/Seaborn** for plotting the heatmap and PCA visualizations.\n",
    "\n",
    "* **What the Code Revealed: RNNs with the \"Lumina Codex\"**\n",
    "\n",
    "    Our tests aimed to evaluate Semantic Similarity, Contextual Understanding, Clustering Quality, the model's handling of its vocabulary, and its basic trainability on this new, richer corpus.\n",
    "\n",
    "    1. **Semantic Similarity & Contextual Understanding (via Heatmap & Specific Pairs):**\n",
    "        We extracted hidden states representing keywords within their contexts. The cosine similarities between these contextual keyword embeddings revealed both strengths and limitations.\n",
    "        \n",
    "        * **Mercury's Multiple Contexts:** The most striking finding was how the RNN handled the word \"mercury\" across its six different contexts in the Lumina Codex:\n",
    "            * **mercury_mythology_1** and **mercury_planet_1** showed very high similarity (0.96), despite referring to completely different concepts (Roman god vs. planet)\n",
    "            * **mercury_metal_1** showed moderate similarity to mythology (0.32) and planet contexts (0.29)\n",
    "            * **mercury_metal_2** showed higher similarity to mythology (0.53) and planet contexts (0.47)\n",
    "            * **mercury_toy_1** maintained moderate connections to mythology (0.33) and planet contexts (0.30)\n",
    "            * **mercury_planet_2** showed strong similarity to mercury_planet_1 (0.48) as expected\n",
    "            \n",
    "        * **Observation from Heatmap:** The heatmap reveals the RNN's struggle with polysemy. Despite \"mercury\" appearing in vastly different contexts (mythology, astronomy, chemistry, toys), the model produces relatively similar embeddings for all instances, with similarities ranging from 0.29 to 0.96. This suggests the RNN is heavily influenced by the shared word \"mercury\" rather than effectively differentiating based on surrounding context.\n",
    "\n",
    "        * **Specific Similarities from General Keywords:**\n",
    "            * \"lumina\" and \"star\" showed moderate similarity (0.41), reflecting their astronomical connection\n",
    "            * \"mercury\" and \"thoth\" exhibited strong similarity (0.62), correctly capturing their mythological relationship\n",
    "            * \"ferrus\" and \"iron\" showed surprisingly low similarity (0.16), despite being related metal terms\n",
    "            * \"ignis\" and \"mercury\" showed moderate similarity (0.41), possibly due to alchemical associations\n",
    "\n",
    "    2. **Clustering Quality (via PCA Plot):**\n",
    "        The PCA visualizations reveal interesting patterns in how the RNN organizes semantic space:\n",
    "        \n",
    "        * **Mercury Context Clustering:** In the mercury-focused PCA plot, we observe:\n",
    "            * mercury_mythology_1 and mercury_planet_1 cluster very closely despite different meanings\n",
    "            * mercury_metal_1 appears isolated in the lower right\n",
    "            * mercury_toy_1 is separated in the lower left\n",
    "            * The two planet contexts (mercury_planet_1 and mercury_planet_2) don't cluster as tightly as expected\n",
    "            * Standalone keywords (planet, metal, mythology, toy) don't align well with their corresponding mercury contexts\n",
    "        \n",
    "        * **General Keyword Clustering:** The broader keyword PCA shows:\n",
    "            * Celestial terms (star, sun, nebula) cluster in the lower portion but aren't tightly grouped\n",
    "            * Elements (mercury, ferrus, iron, ignis) are scattered across the space\n",
    "            * \"thoth\" (mythology) appears isolated at the top, far from \"mercury\"\n",
    "            * No clear semantic clusters emerge, indicating the RNN struggles to create well-defined conceptual regions\n",
    "\n",
    "    3. **Contextual Understanding Limitations:**\n",
    "        The RNN's handling of \"mercury\" exemplifies its fundamental limitation with context:\n",
    "        * Despite appearing in sentences about mythology (\"Mercury, messenger of the gods\"), astronomy (\"Mercury orbits closest to Lumina\"), chemistry (\"liquid mercury metal\"), and toys (\"mercury toy from Earth\"), the model fails to create sufficiently distinct representations\n",
    "        * The high similarity between mythology and planet contexts (0.96) suggests the model relies heavily on the word itself rather than contextual disambiguation\n",
    "        * This demonstrates the SimpleRNN's limited ability to maintain and utilize broader context beyond immediate surrounding words\n",
    "\n",
    "\n",
    "    5. **Handling of Vocabulary Terms:**\n",
    "        * The tokenizer successfully captured domain-specific terms from the Lumina Codex (lumina, ignis, ferrus, thoth, etc.)\n",
    "        * However, the quality of learned representations varies significantly, with some related terms (ferrus/iron) showing unexpectedly low similarity\n",
    "        * The model cannot generate meaningful representations for words outside the Lumina Codex vocabulary\n",
    "\n",
    "    6. **Trainability and Adaptation:**\n",
    "        * The model successfully trained on the Lumina Codex, learning to associate terms within this specific domain\n",
    "        * However, the quality of learned representations reveals the architecture's limitations:\n",
    "            * Poor disambiguation of polysemous words (mercury)\n",
    "            * Inconsistent semantic similarities (ferrus/iron low, mercury/thoth high)\n",
    "            * Lack of clear semantic clustering in the embedding space\n",
    "\n",
    "* **Conclusion on RNNs from \"Lumina Codex\" Exploration:**\n",
    "    This hands-on experiment with a SimpleRNN on the \"Lumina Codex\" text concretely demonstrates both the capabilities and severe limitations of basic RNN architectures. While the model can learn sequential patterns and produce contextual representations, it fundamentally struggles with:\n",
    "    \n",
    "    1. **Polysemy**: The \"mercury\" example starkly illustrates how RNNs fail to adequately distinguish between different meanings of the same word based on context\n",
    "    2. **Long-range dependencies**: The model cannot effectively use broader document context to inform word representations\n",
    "    3. **Semantic coherence**: The lack of clear clustering in PCA space indicates poor semantic organization\n",
    "    \n",
    "    These limitations—particularly evident in the mercury polysemy problem—motivated the development of more sophisticated architectures and embedding methods that could better capture both static word meanings and dynamic contextual variations, leading to innovations like Word2Vec, and eventually, transformer-based models that can effectively handle such contextual nuances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
