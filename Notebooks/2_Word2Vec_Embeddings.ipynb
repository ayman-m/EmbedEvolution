{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616bbb91",
   "metadata": {},
   "source": [
    "# EmbedEvolution Stage 2: Word2Vec Embeddings\n",
    "\n",
    "Welcome to the second stage of EmbedEvolution! We now explore Word2Vec, a highly influential model that learns static word embeddings from local word co-occurrence statistics. Unlike RNNs which process sequences to generate potentially contextual (though limited) representations, Word2Vec assigns a single, fixed vector to each word in the vocabulary.\n",
    "\n",
    "**Goal:** Understand how Word2Vec captures semantic similarities, its efficiency, and its inherent limitations, particularly its static nature and lack of true contextual understanding compared to sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28060f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries. We'll use `gensim` for Word2Vec.\n",
    "`nltk` is useful for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34735d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # Ensure this is imported to be used below\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity # For manual similarity if needed\n",
    "\n",
    "# Download nltk resources if not already present (run once)\n",
    "try:\n",
    "    # Attempt to use a function that requires 'punkt' to see if it's available\n",
    "    _ = sent_tokenize(\"This is a test sentence.\")\n",
    "    print(\"NLTK 'punkt' resource seems to be available.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' resource not found. Downloading...\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"NLTK 'punkt' resource downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during NLTK setup: {e}\")\n",
    "\n",
    "print(\"Gensim version:\", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54173d2a",
   "metadata": {},
   "source": [
    "## 2. Define and Preprocess the Corpus\n",
    "\n",
    "We'll use the same \"Alice's Adventures in Wonderland\" text as in the RNN stage for comparison.\n",
    "Word2Vec typically expects input as a list of sentences, where each sentence is a list of tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do: once or twice she had peeped into the book her sister was reading,\n",
    "but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice\n",
    "'without pictures or conversation?' So she was considering in her own mind (as well as she could,\n",
    "for the hot day made her feel very sleepy and stupid), whether the pleasure of making a\n",
    "daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
    "when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "\n",
    "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way\n",
    "to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!'\n",
    "(when she thought it over afterwards, it occurred to her that she ought to have wondered at this,\n",
    "but at the time it all seemed quite natural); but when the Rabbit actually TOOK A WATCH OUT OF ITS\n",
    "WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed\n",
    "across her mind that she had never before seen a rabbit with either a waistcoat-pocket,\n",
    "or a watch to take out of it, and burning with curiosity, she ran across the field after it,\n",
    "and fortunately was just in time to see it pop down a large rabbit-hole under some hedge.\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text: lowercase and remove special characters (keeping basic punctuation for sentence tokenization)\n",
    "text_cleaned_for_sents = text.lower()\n",
    "text_cleaned_for_sents = re.sub(r'[^a-z\\s\\.\\?\\!]', '', text_cleaned_for_sents) # Keep sentence terminators\n",
    "text_cleaned_for_sents = re.sub(r'\\s+', ' ', text_cleaned_for_sents).strip()\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences_raw = sent_tokenize(text_cleaned_for_sents)\n",
    "\n",
    "# Tokenize sentences into words and further clean words\n",
    "tokenized_sentences = []\n",
    "for sentence in sentences_raw:\n",
    "    # Remove punctuation from words now\n",
    "    sentence_no_punct = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    words = word_tokenize(sentence_no_punct)\n",
    "    words = [word for word in words if word] # Remove empty strings\n",
    "    if words: # Only add non-empty lists of words\n",
    "        tokenized_sentences.append(words)\n",
    "\n",
    "print(f\"Number of sentences: {len(tokenized_sentences)}\")\n",
    "print(\"Sample tokenized sentence:\", tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc795e8",
   "metadata": {},
   "source": [
    "## 3. Train Word2Vec Model\n",
    "\n",
    "We'll use Gensim's `Word2Vec` implementation. Key parameters:\n",
    "* `sentences`: Our tokenized text.\n",
    "* `vector_size`: Dimensionality of the word vectors.\n",
    "* `window`: The maximum distance between the current and predicted word within a sentence.\n",
    "* `min_count`: Ignores all words with total frequency lower than this.\n",
    "* `sg`: Training algorithm: 1 for skip-gram; 0 for CBOW. Skip-gram often works better for infrequent words.\n",
    "* `workers`: Number of worker threads to train the model (parallelism).\n",
    "* `epochs`: Number of iterations (epochs) over the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e16b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # Or 50, to match RNN if desired for some comparisons\n",
    "window_size = 5\n",
    "min_word_count = 1 # For our small corpus, keep all words\n",
    "num_workers = 3 # Adjust based on your CPU cores\n",
    "training_algorithm = 1 # 1 for Skip-gram, 0 for CBOW\n",
    "num_epochs = 50 # Increase for better quality on larger datasets\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences,\n",
    "                          vector_size=embedding_dim,\n",
    "                          window=window_size,\n",
    "                          min_count=min_word_count,\n",
    "                          workers=num_workers,\n",
    "                          sg=training_algorithm,\n",
    "                          epochs=num_epochs)\n",
    "\n",
    "# Build vocabulary (this happens during instantiation if sentences are provided)\n",
    "# word2vec_model.build_vocab(tokenized_sentences) # Not strictly needed if sentences passed to constructor\n",
    "# Train the model (also happens during instantiation if sentences are provided)\n",
    "# word2vec_model.train(tokenized_sentences, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "print(\"Word2Vec model trained.\")\n",
    "print(f\"Vocabulary size: {len(word2vec_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202d9a1",
   "metadata": {},
   "source": [
    "## 4. Explore Word Embeddings\n",
    "\n",
    "Now that the model is trained, we can inspect the learned vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ce684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core words for comparison (derived from RNN stage analysis)\n",
    "core_comparison_words = ['alice', 'rabbit', 'sister', 'book', 'watch', 'curiosity', 'sleepy', 'hedge']\n",
    "words_in_vocab = sorted([word for word in core_comparison_words if word in word2vec_model.wv.index_to_key]) # Sort for consistent matrix output\n",
    "\n",
    "if not words_in_vocab:\n",
    "    print(\"Warning: None of the core comparison words are in the Word2Vec vocabulary. Skipping specific comparisons.\")\n",
    "else:\n",
    "    print(f\"Core words found in vocabulary for comparison: {words_in_vocab}\")\n",
    "\n",
    "    # Get the vector for a sample word\n",
    "    try:\n",
    "        sample_word = words_in_vocab[0]\n",
    "        sample_vector = word2vec_model.wv[sample_word]\n",
    "        print(f\"\\nVector for '{sample_word}' (first 10 dims): {sample_vector[:10]}\")\n",
    "        print(f\"Shape of '{sample_word}' vector: {sample_vector.shape}\")\n",
    "    except (KeyError, IndexError):\n",
    "        print(f\"\\nCould not retrieve vector for a sample word from {words_in_vocab}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Pairwise Cosine Similarities for Core Words ---\")\n",
    "    # Create a mini similarity matrix for these words\n",
    "    if len(words_in_vocab) > 1:\n",
    "        similarity_matrix_core = np.zeros((len(words_in_vocab), len(words_in_vocab)))\n",
    "        for i, word1 in enumerate(words_in_vocab):\n",
    "            for j, word2 in enumerate(words_in_vocab):\n",
    "                similarity_matrix_core[i, j] = word2vec_model.wv.similarity(word1, word2)\n",
    "\n",
    "        print(\"       \", \" \".join(f\"{w[:6]:>6}\" for w in words_in_vocab)) # Print header\n",
    "        for i, word1 in enumerate(words_in_vocab):\n",
    "            print(f\"{word1[:6]:>6}\", end=\" \")\n",
    "            for j, word2 in enumerate(words_in_vocab): # type: ignore\n",
    "                print(f\"{similarity_matrix_core[i, j]:>6.3f}\", end=\" \")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Not enough core words in vocab to create a similarity matrix.\")\n",
    "\n",
    "\n",
    "    # Highlight specific similarities (if words exist)\n",
    "    print(\"\\n--- Specific Highlighted Similarities ---\")\n",
    "    pairs_to_check = [('alice', 'rabbit'), ('alice', 'sister'), ('book', 'watch'), ('sleepy', 'curiosity')]\n",
    "    for w1, w2 in pairs_to_check:\n",
    "        if w1 in word2vec_model.wv and w2 in word2vec_model.wv:\n",
    "            sim = word2vec_model.wv.similarity(w1, w2)\n",
    "            print(f\"Similarity between '{w1}' and '{w2}': {sim:.4f}\")\n",
    "        else:\n",
    "            print(f\"Cannot compare '{w1}' and '{w2}', one or both not in vocabulary.\")\n",
    "\n",
    "# General exploration (most similar)\n",
    "print(\"\\n--- General Word Similarity Exploration ---\")\n",
    "for word_to_explore in ['alice', 'rabbit', 'book']: # A few interesting general words\n",
    "    if word_to_explore in word2vec_model.wv:\n",
    "        try:\n",
    "            similar_words = word2vec_model.wv.most_similar(word_to_explore, topn=3)\n",
    "            print(f\"Words most similar to '{word_to_explore}': {similar_words}\")\n",
    "        except KeyError:\n",
    "            print(f\"'{word_to_explore}' not found in vocab to check general similarity.\")\n",
    "    else:\n",
    "        print(f\"'{word_to_explore}' not in vocabulary for general exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6c5bd",
   "metadata": {},
   "source": [
    "## 5. Select Words and Visualize Embeddings with PCA\n",
    "\n",
    "We'll pick some words from our vocabulary and visualize their relationships using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe769a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for words actually in the model's vocabulary\n",
    "# Ensure core_comparison_words is defined from the previous cell or uncomment above\n",
    "if 'core_comparison_words' not in locals():\n",
    "    core_comparison_words = ['alice', 'rabbit', 'sister', 'book', 'watch', 'curiosity', 'sleepy', 'hedge']\n",
    "    print(\"Warning: 'core_comparison_words' was not defined, using default set for plot.\")\n",
    "    \n",
    "\n",
    "words_in_vocab_for_plot = sorted([word for word in core_comparison_words if word in word2vec_model.wv.index_to_key])\n",
    "\n",
    "# Fallback if too few core words are available (e.g., less than 3 words to plot)\n",
    "if len(words_in_vocab_for_plot) < 3:\n",
    "    print(f\"Warning: Only {len(words_in_vocab_for_plot)} core comparison words found in vocabulary: {words_in_vocab_for_plot}.\")\n",
    "    print(\"Attempting to use a fallback set of words for PCA plot by adding more words from vocabulary.\")\n",
    "\n",
    "    # Add some of the most frequent words from the vocabulary as fallback\n",
    "    # (up to a certain number, ensuring they are not already included)\n",
    "    num_fallback_words_to_add = 10 - len(words_in_vocab_for_plot) # Try to get up to 10 words total\n",
    "    if num_fallback_words_to_add > 0:\n",
    "        vocab_list = list(word2vec_model.wv.index_to_key)\n",
    "        for i in range(min(num_fallback_words_to_add, len(vocab_list))):\n",
    "            if vocab_list[i] not in words_in_vocab_for_plot:\n",
    "                words_in_vocab_for_plot.append(vocab_list[i])\n",
    "        words_in_vocab_for_plot = sorted(list(set(words_in_vocab_for_plot))) # Make unique and sort\n",
    "\n",
    "    if len(words_in_vocab_for_plot) < 2: # Still not enough for PCA\n",
    "         print(\"Fallback also resulted in too few words. Skipping PCA plot.\")\n",
    "         words_in_vocab_for_plot = [] # Empty list to ensure plotting logic is skipped\n",
    "\n",
    "print(f\"\\nWords selected for PCA plot: {words_in_vocab_for_plot}\")\n",
    "\n",
    "\n",
    "# Proceed with plotting if we have at least 2 words (PCA needs n_samples >= n_components)\n",
    "if words_in_vocab_for_plot and len(words_in_vocab_for_plot) >= 2:\n",
    "    embedding_matrix = np.array([word2vec_model.wv[word] for word in words_in_vocab_for_plot])\n",
    "    print(f\"\\nShape of embedding matrix for PCA: {embedding_matrix.shape}\")\n",
    "\n",
    "    # PCA Transformation\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 10)) # Adjusted figure size for better layout with text\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=60) # Slightly larger points\n",
    "\n",
    "    # Annotate points\n",
    "    for i, word in enumerate(words_in_vocab_for_plot):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=9)\n",
    "\n",
    "    plt.title('Word2Vec Embeddings Visualization (PCA)', fontsize=16)\n",
    "    plt.xlabel('PCA Component 1', fontsize=12)\n",
    "    plt.ylabel('PCA Component 2', fontsize=12)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add summary text (this is crucial for comparison)\n",
    "    summary_text = \"\"\"\n",
    "    **Word2Vec Embedding Summary:**\n",
    "    - **Strengths:** Efficiently learns semantic relationships from local word co-occurrences.\n",
    "      Words appearing in similar contexts (e.g., 'alice', 'sister', 'rabbit' might cluster based\n",
    "      on narrative roles in this specific text) tend to have similar vectors. Can capture\n",
    "      analogies if vocabulary and corpus are rich.\n",
    "    - **Context Awareness (Limitation):** Produces *static* embeddings. Each word has a single,\n",
    "      fixed vector regardless of its specific meaning or grammatical role in a sentence.\n",
    "      For example, the word 'rabbit' has only ONE vector, unlike in the RNN stage where different\n",
    "      contexts ('rabbit1' vs 'rabbit2') could yield different hidden state representations.\n",
    "    - **Long-term Dependencies (Limitation):** Does not directly model long-term dependencies or\n",
    "      word order beyond its fixed context window (`window_size`). This is unlike RNNs which\n",
    "      *attempt* to process entire sequences, or Transformer models which excel at capturing such dependencies.\n",
    "    - **Limitations Compared to RNN/Future Models:** The static nature is a key differentiator.\n",
    "      Word2Vec doesn't provide truly contextualized representations like RNN hidden states or\n",
    "      BERT embeddings. It also typically struggles with Out-Of-Vocabulary (OOV) words unless\n",
    "      extensions like FastText (which uses subword information) are employed.\n",
    "    - **Overall:** Word2Vec is a foundational model for learning dense word representations,\n",
    "      offering significant improvements over sparse count-based vectors by embedding semantic\n",
    "      meaning directly into the vector space. It forms a baseline for understanding more\n",
    "      complex embedding techniques.\n",
    "    \"\"\"\n",
    "    # Adjust figtext y-position if needed based on your plot's appearance\n",
    "    plt.figtext(0.5, -0.18, summary_text, ha=\"center\", fontsize=10, bbox={\"facecolor\":\"lightgreen\", \"alpha\":0.2, \"pad\":5}, wrap=True)\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.4) # Adjust layout to make more space for the summary text below\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Could not create PCA plot: Not enough words found in vocabulary for plotting (need at least 2).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f521ada",
   "metadata": {},
   "source": [
    "## 6. Attempting Text Autocompletion with Word2Vec (Demonstrating Limitations)\n",
    "\n",
    "Word2Vec is designed to learn static embeddings that capture semantic relationships between words, not to model sequential probabilities like an RNN language model.\n",
    "However, to illustrate its different nature and its limitations for sequence generation tasks like autocompletion, we can try a naive approach: given a seed text, we'll take the last word and find its most similar word in the Word2Vec model to append next.\n",
    "We expect this to primarily generate lists of related words rather than coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_word2vec_naive(w2v_model_or_vectors, seed_text_str, num_words_to_generate):\n",
    "    \"\"\"\n",
    "    Naively \"generates\" text by appending the most similar word to the last word in the sequence.\n",
    "    \"\"\"\n",
    "    generated_text = seed_text_str.lower()\n",
    "    current_word_list = generated_text.split()\n",
    "\n",
    "    # Determine if we have the full model or just KeyedVectors\n",
    "    if hasattr(w2v_model_or_vectors, 'wv'):\n",
    "        vectors = w2v_model_or_vectors.wv  # It's a full Word2Vec model object\n",
    "    else:\n",
    "        vectors = w2v_model_or_vectors      # Assume it's already a KeyedVectors instance\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        if not current_word_list:\n",
    "            print(\"Error: Current word list is empty.\")\n",
    "            break\n",
    "        \n",
    "        last_word = current_word_list[-1]\n",
    "\n",
    "        if last_word in vectors: # Use the 'vectors' object here\n",
    "            try:\n",
    "                similar_words_tuples = vectors.most_similar(last_word, topn=5) # And here\n",
    "                next_word = None\n",
    "                for sim_word, score in similar_words_tuples:\n",
    "                    if sim_word not in current_word_list[-3:]: \n",
    "                        next_word = sim_word\n",
    "                        break\n",
    "                if next_word is None and similar_words_tuples:\n",
    "                     next_word = similar_words_tuples[0][0]\n",
    "                \n",
    "                if not next_word:\n",
    "                    print(f\"Warning: No suitable similar words found for '{last_word}'. Stopping.\")\n",
    "                    break\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Word '{last_word}' not in model vocabulary during most_similar. Stopping.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred finding similar word for '{last_word}': {e}. Stopping.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: Last word '{last_word}' not in Word2Vec vocabulary. Stopping generation.\")\n",
    "            break\n",
    "            \n",
    "        generated_text += \" \" + next_word\n",
    "        current_word_list.append(next_word)\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Test \"autocompletion\" with Word2Vec\n",
    "if 'word2vec_model' in locals() and word2vec_model is not None:\n",
    "    seed_texts_for_autocomplete = [\n",
    "        \"alice was\",\n",
    "        \"the white rabbit\",\n",
    "        \"alice thought it was very\",\n",
    "        \"her sister on the bank and\", # Longer seed\n",
    "        \"burning with curiosity she ran\" # Even longer seed\n",
    "    ]\n",
    "    \n",
    "    num_generated_words_w2v = 10 # Generate 10 words\n",
    "\n",
    "    print(\"\\n--- Word2Vec Naive 'Autocompletion' Results ---\")\n",
    "    for seed in seed_texts_for_autocomplete:\n",
    "        generated_sequence_w2v = generate_text_word2vec_naive(word2vec_model, seed, num_generated_words_w2v)\n",
    "        print(f\"Seed: '{seed}'\\n  -> Generated: '{generated_sequence_w2v}'\\n\")\n",
    "else:\n",
    "    print(\"Word2Vec model not available for autocompletion test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8ce03",
   "metadata": {},
   "source": [
    "## 7. Using a Pre-trained Word2Vec Model\n",
    "\n",
    "To see the difference a large and diverse training corpus makes, let's load a well-known pre-trained Word2Vec model. Google's model trained on a part of the Google News dataset contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41306837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_pretrained = 'word2vec-google-news-300' # This is a large model\n",
    "\n",
    "try:\n",
    "    print(f\"Loading pre-trained Word2Vec model: '{model_name_pretrained}'...\")\n",
    "    # The device argument is not directly used by Word2Vec model loading in Gensim as it's CPU-based.\n",
    "    pretrained_w2v_model = api.load(model_name_pretrained)\n",
    "    print(f\"Successfully loaded pre-trained Word2Vec model.\")\n",
    "    print(f\"Vocabulary size: {len(pretrained_w2v_model.key_to_index)}\") # In older gensim, it might be len(pretrained_w2v_model.vocab)\n",
    "    print(f\"Vector dimension: {pretrained_w2v_model.vector_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pre-trained Word2Vec model: {e}\")\n",
    "    print(\"Please ensure you have an internet connection for the download.\")\n",
    "    pretrained_w2v_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487486df",
   "metadata": {},
   "source": [
    "## 8. Explore Word Embeddings (from Pre-trained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b97fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_w2v_model:\n",
    "    core_comparison_words_alice = ['alice', 'rabbit', 'sister', 'book', 'watch', 'curiosity', 'sleepy', 'hedge']\n",
    "    # Add some general words that are definitely in Google News\n",
    "    general_words_for_exploration = ['king', 'queen', 'man', 'woman', 'france', 'paris', 'germany', 'berlin']\n",
    "    \n",
    "    words_to_test_pretrained = core_comparison_words_alice + general_words_for_exploration\n",
    "    \n",
    "    print(\"\\n--- Exploring Pre-trained Word2Vec Embeddings ---\")\n",
    "    \n",
    "    # Check some Alice words\n",
    "    for word in ['alice', 'rabbit', 'hatter']: # 'hatter' might be interesting too\n",
    "        if word in pretrained_w2v_model:\n",
    "            print(f\"\\nWords most similar to '{word}' (pre-trained):\")\n",
    "            try:\n",
    "                print(pretrained_w2v_model.most_similar(word, topn=5))\n",
    "            except KeyError:\n",
    "                print(f\"'{word}' present but error in most_similar (rare).\")\n",
    "        else:\n",
    "            print(f\"'{word}' not in pre-trained vocabulary.\")\n",
    "            \n",
    "    # Similarity\n",
    "    print(\"\\n--- Specific Similarities (Pre-trained) ---\")\n",
    "    if 'king' in pretrained_w2v_model and 'queen' in pretrained_w2v_model:\n",
    "        print(f\"Similarity between 'king' and 'queen': {pretrained_w2v_model.similarity('king', 'queen'):.4f}\")\n",
    "    if 'man' in pretrained_w2v_model and 'woman' in pretrained_w2v_model:\n",
    "        print(f\"Similarity between 'man' and 'woman': {pretrained_w2v_model.similarity('man', 'woman'):.4f}\")\n",
    "    if 'france' in pretrained_w2v_model and 'paris' in pretrained_w2v_model:\n",
    "        print(f\"Similarity between 'france' and 'paris': {pretrained_w2v_model.similarity('france', 'paris'):.4f}\")\n",
    "    if 'alice' in pretrained_w2v_model and 'wonderland' in pretrained_w2v_model: # wonderland might be a good test\n",
    "        print(f\"Similarity between 'alice' and 'wonderland': {pretrained_w2v_model.similarity('alice', 'wonderland'):.4f}\")\n",
    "    else:\n",
    "        print(\"'wonderland' might not be in vocab as a single token or common enough.\")\n",
    "\n",
    "\n",
    "    # Analogies\n",
    "    print(\"\\n--- Word Analogies (Pre-trained) ---\")\n",
    "    try:\n",
    "        analogy = pretrained_w2v_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "        print(f\"'king' - 'man' + 'woman' = {analogy}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Could not perform analogy, word not in pre-trained vocabulary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        analogy_capital = pretrained_w2v_model.most_similar(positive=['berlin', 'france'], negative=['germany'], topn=1)\n",
    "        print(f\"'berlin' - 'germany' + 'france' = {analogy_capital}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Could not perform capital analogy, word not in pre-trained vocabulary: {e}\")\n",
    "else:\n",
    "    print(\"Pre-trained Word2Vec model not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f45da0",
   "metadata": {},
   "source": [
    "## 9. Select Words and Visualize Embeddings with PCA (from Pre-trained Model)\n",
    "\n",
    "Now, let's visualize some of these high-quality pre-trained Word2Vec embeddings. We'll use our `core_comparison_words_alice` (words from the Alice text) if they exist in the pre-trained model's vocabulary, and also add some general words for which the pre-trained model should have strong representations. The plot should ideally show more distinct semantic clustering compared to the model trained on our very small \"Alice\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pretrained_w2v_model' in locals() and pretrained_w2v_model is not None:\n",
    "\n",
    "    # Words from Alice text, plus some general words for better illustration with pre-trained model\n",
    "    core_comparison_words_alice = ['alice', 'rabbit', 'sister', 'book', 'watch', 'curiosity', 'sleepy', 'hedge']\n",
    "    \n",
    "    words_for_plot_combined = core_comparison_words_alice\n",
    "    \n",
    "    # Filter for words actually in the pre-trained model's vocabulary\n",
    "    # The pre-trained model object itself is the KeyedVectors instance\n",
    "    words_in_vocab_for_plot_pretrained = sorted(\n",
    "        [word for word in words_for_plot_combined if word in pretrained_w2v_model.key_to_index]\n",
    "    )\n",
    "\n",
    "    # Fallback if too few words are available (less likely with pre-trained model but good practice)\n",
    "    if len(words_in_vocab_for_plot_pretrained) < 3:\n",
    "        print(f\"Warning: Only {len(words_in_vocab_for_plot_pretrained)} selected words found in pre-trained vocabulary: {words_in_vocab_for_plot_pretrained}.\")\n",
    "        print(\"Attempting to use a fallback set of common words for PCA plot.\")\n",
    "        # Fallback: select some very common English words\n",
    "        fallback_words = ['cat', 'dog', 'house', 'tree', 'car', 'road', 'love', 'hate', 'big', 'small']\n",
    "        \n",
    "        # Add fallback words that are in the vocab and not already included\n",
    "        for fw in fallback_words:\n",
    "            if fw in pretrained_w2v_model.key_to_index and fw not in words_in_vocab_for_plot_pretrained:\n",
    "                words_in_vocab_for_plot_pretrained.append(fw)\n",
    "        words_in_vocab_for_plot_pretrained = sorted(list(set(words_in_vocab_for_plot_pretrained)))\n",
    "\n",
    "        if len(words_in_vocab_for_plot_pretrained) < 2:\n",
    "             print(\"Fallback also resulted in too few words. Skipping PCA plot for pre-trained model.\")\n",
    "             words_in_vocab_for_plot_pretrained = []\n",
    "\n",
    "    print(f\"\\nWords selected for PCA plot (pre-trained): {words_in_vocab_for_plot_pretrained}\")\n",
    "\n",
    "    # Proceed with plotting if we have at least 2 words\n",
    "    if words_in_vocab_for_plot_pretrained and len(words_in_vocab_for_plot_pretrained) >= 2:\n",
    "        # Get embeddings from the pre-trained model\n",
    "        embedding_matrix_pretrained = np.array(\n",
    "            [pretrained_w2v_model[word] for word in words_in_vocab_for_plot_pretrained]\n",
    "        )\n",
    "        print(f\"\\nShape of embedding matrix for PCA (pre-trained): {embedding_matrix_pretrained.shape}\")\n",
    "\n",
    "        # PCA Transformation\n",
    "        pca_pretrained = PCA(n_components=2)\n",
    "        embeddings_2d_pretrained = pca_pretrained.fit_transform(embedding_matrix_pretrained)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(16, 12)) # Adjusted for better label visibility\n",
    "        plt.scatter(embeddings_2d_pretrained[:, 0], embeddings_2d_pretrained[:, 1], alpha=0.7, s=60)\n",
    "\n",
    "        # Annotate points\n",
    "        for i, word in enumerate(words_in_vocab_for_plot_pretrained):\n",
    "            plt.annotate(word, (embeddings_2d_pretrained[i, 0], embeddings_2d_pretrained[i, 1]),\n",
    "                         textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=9)\n",
    "\n",
    "        plt.title('Pre-trained Word2Vec (Google News) Embeddings Visualization (PCA)', fontsize=16)\n",
    "        plt.xlabel('PCA Component 1', fontsize=12)\n",
    "        plt.ylabel('PCA Component 2', fontsize=12)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Add summary text\n",
    "        summary_text_pretrained = \"\"\"\n",
    "        **Pre-trained Word2Vec (Google News) Embedding Summary:**\n",
    "        - **Rich Semantics:** Trained on a massive corpus (Google News, ~100 billion words),\n",
    "          these embeddings capture much richer and more general semantic relationships\n",
    "          than models trained on small, specific datasets.\n",
    "        - **Improved Clustering:** Observe how semantically related words cluster more intuitively. The 'Alice' words\n",
    "          will be placed based on their general English meaning.\n",
    "        - **Static Nature:** Despite the higher quality, these are still *static* embeddings.\n",
    "          \"Bank\" would still have one vector, regardless of \"river bank\" vs \"financial bank\".\n",
    "        - **Demonstrates Power of Data:** This plot highlights the significant impact of large and\n",
    "          diverse training data on the quality and generalizability of word embeddings.\n",
    "        \"\"\"\n",
    "        plt.figtext(0.5, -0.08, summary_text_pretrained, ha=\"center\", fontsize=10,\n",
    "                    bbox={\"facecolor\":\"skyblue\", \"alpha\":0.2, \"pad\":5}, wrap=True) # Different color for distinction\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.25) # Adjust layout for summary text\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Could not create PCA plot for pre-trained model: Not enough words found in vocabulary for plotting.\")\n",
    "else:\n",
    "    print(\"Pre-trained Word2Vec model ('pretrained_w2v_model') not loaded. Skipping PCA visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5074a54",
   "metadata": {},
   "source": [
    "## 10. Attempting Text Autocompletion with Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_w2v_model:\n",
    "    print(\"\\n--- Pre-trained Word2Vec Naive 'Autocompletion' Results ---\")\n",
    "    for seed in seed_texts_for_autocomplete: # Using the same seeds as before\n",
    "        generated_sequence_pretrained_w2v = generate_text_word2vec_naive(pretrained_w2v_model, seed, num_generated_words_w2v)\n",
    "        print(f\"Seed: '{seed}'\\n  -> Generated: '{generated_sequence_pretrained_w2v}'\\n\")\n",
    "else:\n",
    "    print(\"Pre-trained Word2Vec model not available for autocompletion test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0faa94",
   "metadata": {},
   "source": [
    "## 11. Word2Vec Stage Conclusion - Performance and Limitations Summary\n",
    "\n",
    "In this notebook, we explored Word2Vec, a pivotal technique for learning static word embeddings. We conducted two sets of experiments: first, training a Word2Vec model from scratch on our small \"Alice in Wonderland\" corpus, and second, utilizing a comprehensive pre-trained Word2Vec model (Google News 300-dim). Through these, we examined word similarity, analogy-solving capabilities, PCA visualizations, and even attempted a naive form of text \"autocompletion\" to highlight its characteristics.\n",
    "\n",
    "**Key Observations from Our Word2Vec Experiments:**\n",
    "\n",
    "1.  **Semantic Similarity & Analogies (Strengths):**\n",
    "    * **Pre-trained Model:** The Google News pre-trained model showcased Word2Vec's classic strength in capturing rich semantic relationships. It produced intuitive similarity scores (e.g., \"king\" was highly similar to \"queen\") and successfully resolved analogies like `king - man + woman â‰ˆ queen`. The PCA plot for the pre-trained model further illustrated a well-structured semantic space, with related concepts clustering meaningfully.\n",
    "    * **Scratch-Trained Model:** Our model trained on the \"Alice\" snippet also learned word vectors. However, due to the very small and specific nature of the corpus, the learned similarities were highly contextual to that text and less generalizable. Cosine similarities between many word pairs were extremely high (often >0.99), and the PCA plot showed a less defined semantic space compared to the pre-trained one. This vividly demonstrated the impact of corpus size and diversity on embedding quality.\n",
    "\n",
    "2.  **Static Nature of Embeddings (Inherent Characteristic & Limitation):**\n",
    "    * Both models produce **static embeddings**: one unique vector per word, regardless of its surrounding context. This means polysemous words (like \"bank\") would have a single, averaged representation. This is a fundamental difference from contextual models like RNNs (whose hidden states vary) or BERT.\n",
    "\n",
    "3.  **Unsuitability for Sequence Generation (Demonstrated by Naive Autocompletion):**\n",
    "    We attempted to \"autocomplete\" text by naively appending the word most similar to the previously generated word.\n",
    "    * **Scratch Model Output Example:** `Seed: 'alice was' -> Generated: 'alice was to and on a book it the out and on'`\n",
    "    * **Pre-trained Model Output Example:** `Seed: 'the white rabbit' -> Generated: 'the white rabbit rabbits animals animal Animal Animals Animals_Peta Animals_PETA PETA PeTA founder_Ingrid_Newkirk'`\n",
    "    * **Interpretation:** These outputs, while amusing, clearly show that Word2Vec is **not a language model designed for sequence generation**. The \"generated\" text lacks grammatical coherence and narrative flow. It primarily strings together words that are semantically close to the *previous* word in isolation, highlighting its focus on word-level semantic similarity rather than sequential probability or grammatical structure. The pre-trained model produced more diverse semantic chains but was equally unsuited for coherent sentence formation. This contrasts with RNNs, which, despite their own flaws in long-term memory, are at least architected to predict based on sequence.\n",
    "\n",
    "4.  **Out-of-Vocabulary (OOV) Words:**\n",
    "    * Our scratch model's vocabulary was limited to the \"Alice\" text. The pre-trained model has a vast vocabulary, but even it can encounter OOV words for highly specialized or very new terms. Standard Word2Vec offers no mechanism to create embeddings for unseen words.\n",
    "\n",
    "**Word2Vec in the \"EmbedEvolution\" Context:**\n",
    "\n",
    "Word2Vec represented a major advancement by providing an **efficient and effective method to learn high-quality static word embeddings**. Its ability to capture semantic relationships like similarity and analogy from large text corpora was transformative for many NLP tasks at the time, such as text classification, information retrieval, and as feature inputs for other models.\n",
    "\n",
    "However, its static nature (one vector per word, inability to disambiguate polysemy) and its focus on individual word representations rather than dynamic contextual understanding or sequence modeling were key limitations that spurred further research.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "The journey continues! Having explored the strengths and limitations of static word embeddings with Word2Vec, the next major leap in our \"EmbedEvolution\" series is to dive into the world of **contextual embeddings** with **BERT (Bidirectional Encoder Representations from Transformers)**. BERT revolutionized NLP by providing word representations that change based on the surrounding words, addressing the key limitation of polysemy and capturing much deeper nuances of language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
