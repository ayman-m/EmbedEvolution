{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993baffb",
   "metadata": {},
   "source": [
    "# EmbedEvolution Stage 7: Instruction-Aware Embeddings with `intfloat/e5-mistral-7b-instruct`\n",
    "\n",
    "Welcome to the final stage of EmbedEvolution! We explore **Instruction-Aware Embedding Models**, using the powerful `intfloat/e5-mistral-7b-instruct`. These models are designed to generate embeddings tailored to specific tasks by understanding and following natural language instructions prepended to the input text. This offers unparalleled flexibility and control.\n",
    "\n",
    "We will:\n",
    "1. Use the pre-trained `intfloat/e5-mistral-7b-instruct` model to demonstrate its core instruction-following capability.\n",
    "2. Analyze its sentence embeddings (using a default instruction) from the \"Lumina Codex\" for semantic similarity and clustering.\n",
    "3. Attempt an *experimental* further fine-tuning of its underlying Transformer base using Masked Language Modeling (MLM) on the \"Lumina Codex\" to observe domain adaptation.\n",
    "4. Compare embeddings before and after this experimental fine-tuning.\n",
    "5. Discuss its characteristics regarding vocabulary, contextual understanding (via instructions), and its nature as an advanced instruction-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e18e55",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "**Note:** `intfloat/e5-mistral-7b-instruct` is a VERY LARGE model (7 Billion parameters).\n",
    "- Ensure you have **significant RAM (e.g., 32GB+, ideally 64GB for fine-tuning)** and **VRAM (e.g., 16GB+, ideally 24GB+ like an A100 or H100 for fine-tuning)**.\n",
    "- Using a powerful GPU (CUDA) is **essential** for reasonable performance, especially for fine-tuning. MPS on Mac M1/M2/M3 will likely be extremely slow or run out of memory for fine-tuning this model. CPU execution will be prohibitively slow.\n",
    "- Consider using a smaller instruction-tuned model (e.g., `hkunlp/instructor-base` or `large`) if resources are limited. This notebook proceeds with `e5-mistral-7b-instruct` as requested, with strong caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f044337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk # For sentence tokenization\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import shutil # For cleaning up saved model directory\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device_str = \"cuda\"\n",
    "    print(\"CUDA is available. Using GPU (CUDA).\")\n",
    "    # Consider torch.cuda.empty_cache() if memory issues persist\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_str = \"mps\"\n",
    "    print(\"MPS is available. Using Apple Silicon GPU (MPS).\")\n",
    "    print(\"WARNING: Fine-tuning a 7B model on MPS will be extremely slow and may lead to memory issues.\")\n",
    "else:\n",
    "    device_str = \"cpu\"\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "    print(\"WARNING: Using CPU for intfloat/e5-mistral-7b-instruct (especially fine-tuning) will be prohibitively slow and require very high RAM.\")\n",
    "device = torch.device(device_str)\n",
    "print(f\"Selected device: {device}\")\n",
    "\n",
    "# Download nltk 'punkt' resource\n",
    "try:\n",
    "    _ = nltk.sent_tokenize(\"Test sentence.\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381317fd",
   "metadata": {},
   "source": [
    "## 2. Define the Corpus: The \"Lumina Codex\"\n",
    "We'll use our detailed \"Lumina Codex and the Solara System\" text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e838f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Lumina Codex and the Solara System: A Tapestry of Ancient Wisdom and Cosmic Discovery\n",
    "In the shadowed halls of the Cairo Museum, a dusty papyrus scroll, cataloged as Papyrus K-37b from the Middle Kingdom, lay forgotten for centuries. Dubbed the Lumina Codex by its discoverers, this fragile relic was initially dismissed as a mythological curiosity, its cryptic hieroglyphs and star charts interpreted as poetic musings of a priestly scribe. Yet, in 2024, a team of linguists and astronomers, led by Dr. Amara Nassar, deciphered its veiled verses, revealing an astonishing truth: the codex described a distant star system with uncanny precision, orbiting a radiant G-type star named the Star of Eternal Radiance—now known as Lumina. This revelation sparked a scientific odyssey, merging ancient Egyptian cosmology with cutting-edge astronomy, as the Solara System emerged from the Nebula Cygnus-X1, nestled in the Orion Spur of the Milky Way Galaxy.\n",
    "\n",
    "The Lumina Codex spoke of Lumina and its ten celestial attendants, organized into poetic regions: the searing Forges of Ra for the inner worlds, the verdant Blessed Belt of Osiris for the habitable zone, the majestic Domains of the Sky Titans for gas giants, and the enigmatic Frozen Outlands for the outer realms. Its star charts, etched with meticulous care, hinted at a cosmic map, with references to the Rivers of Stars—likely the Milky Way—and the Celestial Gardens, evoking the Local Group within the Virgo Supercluster. The codex’s verses, such as “Ten jewels dance in the embrace of the Eternal Radiance, their faces veiled in fire, water, and ice,” seemed to prefigure a system now confirmed by the Cygnus-X1 Deep Sky Array, a fictional next-generation telescope orbiting beyond Earth’s atmosphere.\n",
    "\n",
    "Discovery and Modern Corroboration\n",
    "The Solara System’s discovery began in 2023, when the Cygnus-X1 Deep Sky Array detected subtle wobbles in Lumina’s light, indicating a complex system of orbiting bodies. Located 1,200 light-years away in the Nebula Cygnus-X1, Lumina is a stable, middle-aged G-type star, slightly larger than the Sun, with a luminosity that sustains a diverse array of worlds. As astronomers analyzed the data, they identified ten planets, each with unique characteristics that eerily echoed the Lumina Codex. The parallels were undeniable: the codex’s Forges of Ra matched the inner rocky planets, while the Blessed Belt of Osiris aligned with two habitable worlds teeming with life. The Domains of the Sky Titans and Frozen Outlands described gas giants and icy dwarfs with striking accuracy. The scientific community buzzed with excitement, as linguists and astronomers collaborated to decode the codex’s metaphors, revealing a blend of ancient intuition and cosmic truth.\n",
    "\n",
    "The Solara System: A Celestial Menagerie\n",
    "Lumina: The Star of Eternal Radiance\n",
    "Lumina, a G2V star, radiates a warm, golden light, its stable fusion cycle supporting a system spanning 12 astronomical units. Its magnetic fields are unusually calm, suggesting a long lifespan conducive to life’s evolution. The codex describes Lumina as “the hearth of eternity, whose breath kindles the dance of worlds,” a poetic nod to its life-giving energy.\n",
    "\n",
    "The Forges of Ra: Inner Planets\n",
    "1- Ignis: The closest planet to Lumina, Ignis is a scorched, iron-rich world with a molten surface pocked by ancient impact craters. Its thin atmosphere, rich in sulfur dioxide, glows faintly under Lumina’s intense radiation. The codex calls it “Ra’s Anvil, where molten rivers forge the bones of the cosmos,” reflecting its volcanic past and metallic crust.\n",
    "2- Ferrus: Slightly larger, Ferrus is a rocky planet with vast plains of oxidized iron, giving it a crimson hue. Its surface bears scars of past tectonic activity, with towering cliffs and deep chasms. The codex names it “the Forge of Hephaestus’s Twin,” hinting at its metallic wealth, now confirmed by spectroscopic analysis revealing nickel and cobalt deposits.\n",
    "The Blessed Belt of Osiris: Habitable Zone\n",
    "1- Aqua: A breathtaking ocean world, Aqua is enveloped in turquoise clouds of water vapor and nitrogen. Its surface is 90% liquid water, with archipelagos of coral-like structures hosting complex aquatic ecosystems. Bioluminescent Aquarelles, jellyfish-like creatures with crystalline tentacles, drift in vast schools, their light pulses synchronizing in rhythmic displays. Predatory Thalacynths, eel-like organisms with electromagnetic sensors, hunt in the deep trenches. Aqua’s moon, Thalassa, is an ice-covered world with a subglacial ocean, where astrobiologists hypothesize microbial extremophiles thrive in hydrothermal vents, metabolizing sulfur compounds. The codex describes Aqua as “Osiris’s Chalice, where life swims in the tears of the gods,” and Thalassa as “the frozen veil hiding the spark of creation.”\n",
    "2- Veridia: A super-Earth, Veridia boasts lush continents of bioluminescent flora, such as Luminara trees, which pulse with green and violet light, and Crystalferns, whose fractal leaves refract Lumina’s rays into dazzling spectra. Veridia is home to the Sylvans, sentient, silicon-based life forms resembling ambulatory crystal shrubs. Their bodies, composed of lattice-like structures, shimmer with bioluminescent patterns used for communication. Sylvan society is decentralized, with “groves” of individuals linked via light-based signals, forming a collective consciousness deeply attuned to Veridia’s ecosystem. Their architecture, grown from crystalline minerals, integrates seamlessly with the landscape. The codex calls Veridia “the Garden of Osiris’s Breath,” where “the shining ones weave light into wisdom.”\n",
    "The Domains of the Sky Titans: Gas Giants\n",
    "1- Zephyrus: A massive hydrogen-helium gas giant, Zephyrus dominates the system with its radiant ring system, composed of ice and silicate particles. Its atmosphere swirls with golden storms, driven by intense winds. Among its 47 moons, Io-Prime stands out, a volcanically active world spewing sulfur plumes, likely powered by tidal heating. The codex names Zephyrus “the Sky Titan’s Crown,” its rings “the jeweled girdle of the heavens.”\n",
    "2- Boreas: An ice giant with a deep blue methane atmosphere, Boreas exhibits retrograde rotation and an asymmetrical magnetic field, creating auroras that dance across its poles. Its 22 moons include Erynnis, a rocky moon with methane lakes. The codex describes Boreas as “the Frost Titan, whose breath chills the void,” capturing its icy majesty.\n",
    "The Frozen Outlands: Outer Planets\n",
    "1- Umbriel: A dwarf planet with a charcoal-dark surface, Umbriel’s icy crust is fractured by ancient impacts. Its moon Nyx, a captured object, is rich in organic compounds, hinting at prebiotic chemistry. The codex calls Umbriel “the Shadowed Outcast, guarded by the dark sentinel.”\n",
    "2- Erebus: An icy world with a nitrogen-methane atmosphere, Erebus has a highly elliptical orbit, suggesting a captured origin. Its surface sparkles with frost-covered ridges. The codex names it “the Silent Wanderer, cloaked in eternal frost.”\n",
    "3- Aetheria: The outermost planet, Aetheria is a rogue dwarf with a thin atmosphere of neon and argon. Its moon Lethe exhibits cryovolcanism, spewing ammonia-water mixtures. Astrobiologists speculate that Lethe’s subsurface ocean may harbor microbial life, analogous to Thalassa’s. The codex describes Aetheria as “the Veiled Wanderer, whose dreams freeze in the outer dark,” and Lethe as “the weeping mirror of the cosmos.”\n",
    "4- Nyxara: A small, icy body with a chaotic orbit, Nyxara’s surface is a mosaic of frozen nitrogen and carbon monoxide. The codex calls it “the Lost Jewel, dancing beyond the Titans’ gaze.”\n",
    "Life in the Solara System\n",
    "Aqua’s aquatic ecosystems are a marvel, with Aquarelles forming symbiotic networks with coral-like Hydroskeletons, which filter nutrients from the water. Thalacynths use electromagnetic pulses to stun prey, suggesting an evolutionary arms race. On Thalassa, microbial life is hypothesized based on chemical signatures of sulfur and methane in its subglacial ocean, though no direct evidence exists yet.\n",
    "\n",
    "Veridia’s Sylvans are the system’s crown jewel. Their crystalline bodies, averaging two meters tall, refract light into complex patterns, encoding emotions, ideas, and memories. Their society operates as a “luminous collective,” with no central authority; decisions emerge from synchronized light displays across groves. Sylvan technology manipulates crystalline minerals to create tools and habitats, all in harmony with Veridia’s ecosystem. Their discovery has sparked intense study by linguists decoding their light-based language, revealing a philosophy centered on balance and interconnectedness.\n",
    "\n",
    "On Lethe, cryovolcanic activity suggests a subsurface ocean with potential microbial ecosystems, possibly metabolizing ammonia. Unlike Aqua’s confirmed complex life and Veridia’s sentient Sylvans, life on Thalassa and Lethe remains speculative, driving astrobiological research.\n",
    "\n",
    "Galactic Context\n",
    "The Solara System resides in the Orion Spur, a minor arm of the Milky Way, part of the Local Group within the Virgo Supercluster. The codex’s Rivers of Stars evoke the Milky Way’s spiral arms, while the Celestial Gardens suggest a poetic grasp of the Local Group’s galactic cluster. This cosmic placement underscores Solara’s significance as a microcosm of the universe’s diversity.\n",
    "\n",
    "Ongoing Exploration\n",
    "Scientific teams, including astrobiologists, geologists, and linguists, are studying Solara via the Cygnus-X1 Deep Sky Array and planned probes, such as the Lumina Pathfinder Mission. Challenges include the 1,200-light-year distance, requiring advanced telemetry for data transmission. Sylvan communication poses a unique hurdle, as their light patterns defy traditional linguistic models. Future missions aim to deploy orbiters around Aqua, Veridia, and Lethe to confirm microbial life and study Sylvan culture.\n",
    "\n",
    "A Cosmic Tapestry\n",
    "The Solara System, unveiled through the Lumina Codex and modern astronomy, blends ancient wisdom with scientific discovery. Its worlds—from the fiery Forges of Ra to the icy Frozen Outlands—offer a rich tapestry of environments, life forms, and mysteries. As scientists probe this distant system, the codex’s poetic verses resonate, reminding humanity that the cosmos has long whispered its secrets, awaiting those bold enough to listen.\n",
    "\"\"\"\n",
    "def clean_text_for_embedding_models(input_text):\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text).strip()\n",
    "    return input_text\n",
    "\n",
    "cleaned_full_text_instruct_corpus = clean_text_for_embedding_models(text)\n",
    "sentences_from_corpus_instruct = nltk.sent_tokenize(text)\n",
    "cleaned_sentences_for_instruct_analysis = [clean_text_for_embedding_models(s) for s in sentences_from_corpus_instruct if s.strip()]\n",
    "\n",
    "print(f\"Found {len(cleaned_sentences_for_instruct_analysis)} sentences for Instruction-Aware model analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022afd60",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Instruction-Aware Model (`intfloat/e5-mistral-7b-instruct`)\n",
    "This model is designed to follow natural language instructions prepended to the text.\n",
    "It requires `trust_remote_code=True` for loading its custom Mistral architecture if not natively supported by SentenceTransformer's AutoModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCT_MODEL_NAME = 'intfloat/e5-mistral-7b-instruct'\n",
    "instructor_model_pretrained = None # Initialize\n",
    "\n",
    "try:\n",
    "    # trust_remote_code=True might be necessary if the model uses custom code not yet in Hugging Face transformers\n",
    "    # For many standard architectures wrapped by SentenceTransformer, it might not be needed.\n",
    "    # However, E5-Mistral could have specifics.\n",
    "    instructor_model_pretrained = SentenceTransformer(INSTRUCT_MODEL_NAME, device=device_str, trust_remote_code=True)\n",
    "    print(f\"Successfully loaded PRE-TRAINED Instruction-Aware model: '{INSTRUCT_MODEL_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PRE-TRAINED Instruction-Aware model: {e}\")\n",
    "    print(\"This model is very large. Ensure sufficient VRAM/RAM and correct installation environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7871bc",
   "metadata": {},
   "source": [
    "## 4. Part 1: Analysis with Pre-trained `intfloat/e5-mistral-7b-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2971e4",
   "metadata": {},
   "source": [
    "### 4.1 Demonstrating Instruction-Awareness\n",
    "We take a sample sentence and embed it with different types of instructions/prefixes.\n",
    "The E5 series typically uses \"query: \" and \"passage: \" for retrieval.\n",
    "Instruction-tuned models like this one often respond well to more descriptive instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if instructor_model_pretrained and cleaned_sentences_for_instruct_analysis:\n",
    "    sample_sentence_idx_instruct = min(20, len(cleaned_sentences_for_instruct_analysis) - 1) # Pick a descriptive sentence\n",
    "    if sample_sentence_idx_instruct >=0 :\n",
    "        sample_sentence_instruct = cleaned_sentences_for_instruct_analysis[sample_sentence_idx_instruct]\n",
    "        print(f\"Sample Sentence for Instruction Test: '{sample_sentence_instruct}'\")\n",
    "\n",
    "        # Define instructions. For e5-mistral-7b-instruct, it's often \"Instruct: [task_description] \\n Document: [text]\"\n",
    "        # or specific prefixes like \"query: \" / \"passage: \"\n",
    "        instructions_to_test = {\n",
    "            \"as_passage\": \"passage: \", # E5-style for general document representation\n",
    "            \"as_query_for_discovery\": \"query: What new discoveries were made in the Solara system? \", # E5-style query\n",
    "            \"for_classification_topic\": \"Instruct: Classify the main topic of this text. \\n Document: \",\n",
    "            \"for_similarity_check\": \"Instruct: Represent this sentence to find semantically similar sentences. \\n Sentence: \",\n",
    "            \"no_instruction\": \"\" # Baseline\n",
    "        }\n",
    "\n",
    "        print(f\"\\n--- Embeddings for Sample Sentence with Different Instructions ---\")\n",
    "        sample_embeddings_by_instruction = {}\n",
    "        for key, instruction_prefix in instructions_to_test.items():\n",
    "            text_to_embed = instruction_prefix + sample_sentence_instruct\n",
    "            try:\n",
    "                embedding = instructor_model_pretrained.encode(\n",
    "                    text_to_embed, convert_to_numpy=True, normalize_embeddings=True # E5 models usually require normalization\n",
    "                )\n",
    "                sample_embeddings_by_instruction[key] = embedding\n",
    "                print(f\"Embedding for '{key}' (first 3 dims): {embedding[:3]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding with instruction '{key}': {e}\")\n",
    "                sample_embeddings_by_instruction[key] = None\n",
    "        \n",
    "        # Compare some embeddings\n",
    "        if sample_embeddings_by_instruction.get(\"as_passage\") is not None and \\\n",
    "           sample_embeddings_by_instruction.get(\"as_query_for_discovery\") is not None:\n",
    "            sim_passage_query = cosine_similarity(\n",
    "                sample_embeddings_by_instruction[\"as_passage\"].reshape(1, -1),\n",
    "                sample_embeddings_by_instruction[\"as_query_for_discovery\"].reshape(1, -1)\n",
    "            )[0][0]\n",
    "            print(f\"\\nSim. of SAME sentence with 'passage:' vs 'query: What new discoveries...': {sim_passage_query:.4f}\")\n",
    "        \n",
    "        # PCA of these instruction-varied embeddings for one sentence\n",
    "        plot_labels_one_sent_instruct = [k for k,v in sample_embeddings_by_instruction.items() if v is not None]\n",
    "        embeddings_to_plot_one_sent_instruct = [sample_embeddings_by_instruction[k] for k in plot_labels_one_sent_instruct]\n",
    "\n",
    "        if len(embeddings_to_plot_one_sent_instruct) >= 2:\n",
    "            pca_one_sent_instruct = PCA(n_components=2)\n",
    "            embeddings_2d_one_sent_instruct = pca_one_sent_instruct.fit_transform(np.array(embeddings_to_plot_one_sent_instruct))\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.scatter(embeddings_2d_one_sent_instruct[:, 0], embeddings_2d_one_sent_instruct[:, 1], s=120)\n",
    "            for i, label in enumerate(plot_labels_one_sent_instruct):\n",
    "                plt.annotate(label, (embeddings_2d_one_sent_instruct[i, 0], embeddings_2d_one_sent_instruct[i, 1]),\n",
    "                             textcoords=\"offset points\", xytext=(5,5), ha='center', fontsize=9)\n",
    "            plt.title(f'PCA of One Sentence with Different Instructions ({INSTRUCT_MODEL_NAME})', fontsize=14)\n",
    "            plt.xlabel('PCA Comp 1'); plt.ylabel('PCA Comp 2'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"Not enough sentences in corpus to run prefix test.\")\n",
    "else:\n",
    "    print(\"Instruction-Aware model not loaded or no sentences. Skipping instruction impact demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc103930",
   "metadata": {},
   "source": [
    "### 4.2 Define Sentences for Broader Analysis & Generate Embeddings (Default Instruction)\n",
    "For consistent analysis across sentences, we'll use a general \"passage: \" instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences_for_instruct_analysis = []\n",
    "selected_sentence_labels_instruct_analysis = []\n",
    "# (Using similar selection logic as previous notebooks)\n",
    "# ... (You can copy your sentence selection logic from GTE/SBERT notebook here) ...\n",
    "# For brevity, I'll just take first 15 for this example, you should use your keyword-based selection\n",
    "if cleaned_sentences_for_instruct_analysis:\n",
    "    selected_sentences_for_instruct_analysis = cleaned_sentences_for_instruct_analysis[:min(15, len(cleaned_sentences_for_instruct_analysis))]\n",
    "    selected_sentence_labels_instruct_analysis = [f\"S{i+1}_{s[:10].replace(' ','_')}\" for i,s in enumerate(selected_sentences_for_instruct_analysis)]\n",
    "    print(f\"\\nSelected {len(selected_sentences_for_instruct_analysis)} sentences for broader analysis.\")\n",
    "else:\n",
    "    print(\"No cleaned sentences available for broader analysis.\")\n",
    "\n",
    "# Default instruction for embedding passages\n",
    "default_instruction_e5 = \"passage: \"\n",
    "sentences_with_default_e5_instruction = [default_instruction_e5 + s for s in selected_sentences_for_instruct_analysis]\n",
    "\n",
    "sentence_embeddings_instruct_pt = None # pt for pre-trained\n",
    "if instructor_model_pretrained and sentences_with_default_e5_instruction:\n",
    "    print(f\"\\nGenerating Instruction-Aware embeddings (pre-trained) for {len(sentences_with_default_e5_instruction)} sentences using instruction: '{default_instruction_e5.strip()}'...\")\n",
    "    sentence_embeddings_instruct_pt = instructor_model_pretrained.encode(\n",
    "        sentences_with_default_e5_instruction, convert_to_numpy=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(f\"Generated Instruction-Aware (pre-trained) embeddings. Shape: {sentence_embeddings_instruct_pt.shape if sentence_embeddings_instruct_pt is not None else 'None'}\")\n",
    "else:\n",
    "    print(\"Instruction-Aware model or sentences not ready for default instruction embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c9af2",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate Pre-trained Instruction-Aware Embeddings (Default Instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentence_embeddings_instruct_pt is not None and sentence_embeddings_instruct_pt.shape[0] >= 2:\n",
    "    embedding_matrix_instruct_pt = sentence_embeddings_instruct_pt\n",
    "    plot_labels_instruct_pt = selected_sentence_labels_instruct_analysis\n",
    "\n",
    "    # --- Semantic Similarity (Heatmap) ---\n",
    "    print(\"\\n--- Cosine Similarity Heatmap (Sentences - Pre-trained Instruction-Aware, default instruction) ---\")\n",
    "    similarity_matrix_instruct_pt = np.dot(embedding_matrix_instruct_pt, embedding_matrix_instruct_pt.T)\n",
    "    num_lbl_instruct_pt = len(plot_labels_instruct_pt); fig_w_instruct_pt = max(12,num_lbl_instruct_pt*0.6); fig_h_instruct_pt = max(10,num_lbl_instruct_pt*0.45)\n",
    "    plt.figure(figsize=(fig_w_instruct_pt, fig_h_instruct_pt))\n",
    "    annot_hm_instruct_pt = num_lbl_instruct_pt < 20\n",
    "    sns.heatmap(similarity_matrix_instruct_pt, annot=annot_hm_instruct_pt, cmap='magma', fmt=\".2f\", xticklabels=plot_labels_instruct_pt, yticklabels=plot_labels_instruct_pt, linewidths=.5, cbar_kws={\"shrink\":.8}, vmin=-1, vmax=1)\n",
    "    plt.title(f'Instruction-Aware ({INSTRUCT_MODEL_NAME}) Similarity (Instruction: \"{default_instruction_e5.strip()}\")', fontsize=16)\n",
    "    plt.xticks(rotation=65, ha='right'); plt.yticks(rotation=0); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- Clustering Quality (PCA) ---\n",
    "    print(\"\\n--- PCA Visualization (Sentences - Pre-trained Instruction-Aware, default instruction) ---\")\n",
    "    pca_instruct_pt = PCA(n_components=2)\n",
    "    embeddings_2d_instruct_pt = pca_instruct_pt.fit_transform(embedding_matrix_instruct_pt)\n",
    "    plt.figure(figsize=(fig_w_instruct_pt*0.9, fig_h_instruct_pt*0.9))\n",
    "    plt.scatter(embeddings_2d_instruct_pt[:,0], embeddings_2d_instruct_pt[:,1], alpha=0.7, s=60)\n",
    "    for i,lbl in enumerate(plot_labels_instruct_pt): plt.annotate(lbl,(embeddings_2d_instruct_pt[i,0],embeddings_2d_instruct_pt[i,1]),textcoords=\"offset points\",xytext=(5,5),ha='center')\n",
    "    plt.title(f'Instruction-Aware ({INSTRUCT_MODEL_NAME}) Embeddings (Instruction: \"{default_instruction_e5.strip()}\") - PCA', fontsize=16)\n",
    "    plt.xlabel('PCA Comp 1'); plt.ylabel('PCA Comp 2'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"Not enough embeddings for pre-trained Instruction-Aware model analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff5a5a",
   "metadata": {},
   "source": [
    "### Interpretation Notes for Pre-trained Nomic Embed (with `search_document:`):\n",
    "* **Semantic Similarity & Clustering:** How do sentences from \"Lumina Codex\" relate to each other when embedded for \"document\" purposes? Are the clusters/similarities intuitive for this task type? Compare with GTE/SBERT results.\n",
    "* **Vocabulary & Context:** Handled well by the underlying Transformer. The \"context\" is now also influenced by the task prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69475d0f",
   "metadata": {},
   "source": [
    "## 5. Part 2: Experimental Fine-tuning of `e5-mistral-7b-instruct`'s Base (MLM)\n",
    "\n",
    "**WARNING: Fine-tuning a 7B parameter model like Mistral is extremely resource-intensive.**\n",
    "- This requires significant GPU VRAM (likely 24GB+ per GPU, possibly multiple GPUs for reasonable speed).\n",
    "- It will take a very long time on consumer hardware or even standard cloud GPU instances if not using high-end ones.\n",
    "- This section is primarily for demonstrating the *concept* of domain adaptation. For practical fine-tuning of such large models, specialized hardware, distributed training setups, and techniques like LoRA/QLoRA are typically used.\n",
    "\n",
    "We will proceed with a very small MLM fine-tuning setup for illustrative purposes only. The expectation is to observe if *any* adaptation to the \"Lumina Codex\" occurs, rather than achieving SOTA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bee5ae",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Model and Data for MLM Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_mlm_model_to_finetune = None\n",
    "instruct_mlm_tokenizer = None\n",
    "instruct_mlm_dataloader = None\n",
    "\n",
    "BASE_MISTRAL_FOR_E5 = \"mistralai/Mistral-7B-Instruct-v0.1\" \n",
    "try:\n",
    "    print(f\"Attempting to load tokenizer for base model: {BASE_MISTRAL_FOR_E5}\")\n",
    "    instruct_mlm_tokenizer = AutoTokenizer.from_pretrained(BASE_MISTRAL_FOR_E5, trust_remote_code=True)\n",
    "    # Add padding token if tokenizer doesn't have one (Mistral often doesn't)\n",
    "    if instruct_mlm_tokenizer.pad_token is None:\n",
    "        instruct_mlm_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(f\"Loaded tokenizer '{BASE_MISTRAL_FOR_E5}' for MLM fine-tuning.\")\n",
    "\n",
    "    print(f\"Attempting to load base model for MLM: {BASE_MISTRAL_FOR_E5}\")\n",
    "    # Load the base model configuration\n",
    "    config = AutoConfig.from_pretrained(BASE_MISTRAL_FOR_E5, trust_remote_code=True)\n",
    "    # Try to load it with AutoModelForMaskedLM. This will add a new MLM head if one doesn't exist.\n",
    "    instruct_mlm_model_to_finetune = AutoModelForMaskedLM.from_config(config=config, trust_remote_code=True).to(device)\n",
    "    print(\"INFO: MLM Fine-tuning setup for a 7B model is resource-intensive and complex.\")\n",
    "    print(\"For this notebook, we will proceed by conceptually acknowledging this step.\")\n",
    "    print(\"In a real scenario, one would load the base Mistral model, ensure it has/can have an MLM head, and fine-tune.\")\n",
    "\n",
    "    if instructor_model_pretrained:\n",
    "        instruct_model_finetuned_base = instructor_model_pretrained[0].auto_model \n",
    "        instruct_model_finetuned_base.to(device) # Ensure it's on device\n",
    "        print(\"Using the base of the pre-trained instruction model as a STAND-IN for a fine-tuned base model for demonstration purposes.\")\n",
    "        # In a real scenario, this 'instruct_model_finetuned_base' would be the result of an actual MLM fine-tuning process.\n",
    "    else:\n",
    "        instruct_model_finetuned_base = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing base Mistral model for conceptual MLM fine-tuning: {e}\")\n",
    "    instruct_mlm_model_to_finetune = None; instruct_mlm_tokenizer = None; instruct_model_finetuned_base = None\n",
    "\n",
    "# MLM Dataset Preparation (using the tokenizer for the base model)\n",
    "if instruct_mlm_tokenizer and cleaned_sentences_for_instruct_analysis and instruct_model_finetuned_base is not None:\n",
    "    num_ft_sents_instruct = min(len(cleaned_sentences_for_instruct_analysis), 100) # Even smaller subset for 7B model demo\n",
    "    ft_texts_instruct = cleaned_sentences_for_instruct_analysis[:num_ft_sents_instruct]\n",
    "    if ft_texts_instruct:\n",
    "        mlm_dataset_instruct = MLMDatasetShared(ft_texts_instruct, instruct_mlm_tokenizer, max_length=256) # Shorter max_length for MLM\n",
    "        if len(mlm_dataset_instruct) > 0:\n",
    "            # We won't run the actual fine-tuning loop here due to resource constraints\n",
    "            # but we prepare the dataloader to show the setup.\n",
    "            instruct_mlm_dataloader = DataLoader(\n",
    "                mlm_dataset_instruct, batch_size=1, shuffle=True, collate_fn=mlm_dataset_instruct.data_collator # Batch size 1 for 7B\n",
    "            )\n",
    "            print(f\"Prepared MLM dataset for Instruction model's base with {len(mlm_dataset_instruct)} instances (FINE-TUNING LOOP IS SKIPPED).\")\n",
    "        else: print(\"MLM dataset for Instruction model base is empty.\")\n",
    "    else: print(\"No texts selected for Instruction model MLM fine-tuning.\")\n",
    "else:\n",
    "    if not instruct_model_finetuned_base:\n",
    "        print(\"Base model for fine-tuning not available.\")\n",
    "    else:\n",
    "        print(\"MLM Tokenizer not available for Instruction model fine-tuning.\")\n",
    "    instruct_mlm_dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf62138",
   "metadata": {},
   "source": [
    "### 5.2 Fine-tune Base Model (Conceptual - Loop Skipped for 7B Model Practicality)\n",
    "Due to the immense computational resources required to fine-tune a 7B parameter model like Mistral, we will not execute the training loop in this notebook. We have prepared the data and model structure as a demonstration of how one *would* approach it.\n",
    "For the \"fine-tuned\" evaluation below, we will use the *original pre-trained base* of `intfloat/e5-mistral-7b-instruct` as a stand-in. The purpose is to show the *structure* of comparison, assuming fine-tuning had occurred and produced an `instruct_model_finetuned_base`. In a real experiment with adequate resources, this base model would be the output of an actual MLM training run on the \"Lumina Codex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'instruct_model_finetuned_base' in locals() and instruct_model_finetuned_base is not None:\n",
    "    print(\"Proceeding with 'instruct_model_finetuned_base' (stand-in for actual fine-tuned weights) for evaluation.\")\n",
    "    instruct_model_finetuned_base.eval() # Ensure it's in eval mode\n",
    "else:\n",
    "    print(\"Fine-tuned base for instruction model not available. Skipping fine-tuned evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae9656",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate \"Fine-tuned\" Instruction-Aware Embeddings\n",
    "We reconstruct a SentenceTransformer using the (stand-in) fine-tuned base Mistral model and the original pooling/normalization layers from `intfloat/e5-mistral-7b-instruct`. Then we evaluate sentence embeddings using the same default \"passage: \" instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_instruct_ft = None\n",
    "instructor_model_finetuned_full = None\n",
    "\n",
    "if 'instruct_model_finetuned_base' in locals() and instruct_model_finetuned_base is not None and \\\n",
    "   'instructor_model_pretrained' in locals() and instructor_model_pretrained is not None and \\\n",
    "   'instruct_mlm_tokenizer' in locals() and instruct_mlm_tokenizer is not None:\n",
    "    \n",
    "    print(\"Reconstructing SentenceTransformer with 'fine-tuned' (stand-in) instruction model base...\")\n",
    "    try:\n",
    "        # We need to save the 'instruct_model_finetuned_base' and its 'instruct_mlm_tokenizer'\n",
    "        # to a temporary path so models.Transformer can load them together.\n",
    "        temp_finetuned_instruct_base_path = \"./instruct_finetuned_base_temp\"\n",
    "        \n",
    "        instruct_model_finetuned_base.save_pretrained(temp_finetuned_instruct_base_path)\n",
    "        instruct_mlm_tokenizer.save_pretrained(temp_finetuned_instruct_base_path)\n",
    "        print(f\"Stand-in fine-tuned instruction base model and tokenizer saved to {temp_finetuned_instruct_base_path}\")\n",
    "\n",
    "        # Create the word_embedding_module using the path\n",
    "        word_embedding_module_instruct_ft = models.Transformer(\n",
    "            model_name_or_path=temp_finetuned_instruct_base_path,\n",
    "            max_seq_length=instructor_model_pretrained[0].max_seq_length, # from original SBERT wrapper\n",
    "            trust_remote_code=True # Might be needed if base Mistral has custom code\n",
    "        )\n",
    "        word_embedding_module_instruct_ft.to(device)\n",
    "        \n",
    "        # Get pooling and normalization from the original pre-trained SBERT object\n",
    "        pooling_model_instruct = instructor_model_pretrained[1] # Assumes pooling is the second module\n",
    "        pooling_model_instruct.to(device)\n",
    "        \n",
    "        reconstructed_modules_instruct_ft = [word_embedding_module_instruct_ft, pooling_model_instruct]\n",
    "        \n",
    "        if len(instructor_model_pretrained) > 2 and isinstance(instructor_model_pretrained[2], models.Normalize):\n",
    "            normalize_model_instruct = instructor_model_pretrained[2]\n",
    "            normalize_model_instruct.to(device)\n",
    "            reconstructed_modules_instruct_ft.append(normalize_model_instruct)\n",
    "        elif not any(isinstance(m, models.Normalize) for m in reconstructed_modules_instruct_ft):\n",
    "            print(\"Adding Normalize layer for reconstructed instruction model.\")\n",
    "            reconstructed_modules_instruct_ft.append(models.Normalize())\n",
    "\n",
    "        instructor_model_finetuned_full = SentenceTransformer(modules=reconstructed_modules_instruct_ft, device=device_str)\n",
    "        print(\"Successfully reconstructed SentenceTransformer with 'fine-tuned' (stand-in) instruction model base.\")\n",
    "\n",
    "        # Optional: Clean up\n",
    "        # shutil.rmtree(temp_finetuned_instruct_base_path)\n",
    "\n",
    "        # Proceed with evaluation using the default instruction\n",
    "        if instructor_model_finetuned_full and sentences_with_default_e5_instruction: # From cell 4.2\n",
    "            print(f\"\\nGenerating Instruction-Aware ('fine-tuned' stand-in) embeddings for {len(sentences_with_default_e5_instruction)} sentences...\")\n",
    "            sentence_embeddings_instruct_ft = instructor_model_finetuned_full.encode(\n",
    "                sentences_with_default_e5_instruction, convert_to_numpy=True, normalize_embeddings=True\n",
    "            )\n",
    "            print(f\"Generated Instruction-Aware ('fine-tuned' stand-in) embeddings. Shape: {sentence_embeddings_instruct_ft.shape if sentence_embeddings_instruct_ft is not None else 'None'}\")\n",
    "\n",
    "            if sentence_embeddings_instruct_ft is not None and sentence_embeddings_instruct_ft.shape[0] >= 2:\n",
    "                embedding_matrix_instruct_ft = sentence_embeddings_instruct_ft\n",
    "                plot_labels_instruct_ft = selected_sentence_labels_instruct_analysis # Same labels\n",
    "\n",
    "                print(\"\\n--- Cosine Similarity Heatmap (Sentences - 'Fine-tuned' Instruction-Aware, default instruction) ---\")\n",
    "                similarity_matrix_instruct_ft = np.dot(embedding_matrix_instruct_ft, embedding_matrix_instruct_ft.T)\n",
    "                plt.figure(figsize=(fig_w_instruct_pt, fig_h_instruct_pt)) # Reuse fig sizes from pre-trained\n",
    "                annot_hm_instruct_ft = num_lbl_instruct_pt < 20\n",
    "                sns.heatmap(similarity_matrix_instruct_ft, annot=annot_hm_instruct_ft, cmap='inferno', fmt=\".2f\", xticklabels=plot_labels_instruct_ft, yticklabels=plot_labels_instruct_ft, linewidths=.5, cbar_kws={\"shrink\":.8}, vmin=-1, vmax=1)\n",
    "                plt.title(f'Instruction-Aware ({INSTRUCT_MODEL_NAME}) Similarity (\"Fine-tuned\" Stand-in)', fontsize=16)\n",
    "                plt.xticks(rotation=65, ha='right'); plt.yticks(rotation=0); plt.tight_layout(); plt.show()\n",
    "\n",
    "                print(\"\\n--- PCA Visualization (Sentences - 'Fine-tuned' Instruction-Aware, default instruction) ---\")\n",
    "                pca_instruct_ft = PCA(n_components=2)\n",
    "                embeddings_2d_instruct_ft = pca_instruct_ft.fit_transform(embedding_matrix_instruct_ft)\n",
    "                plt.figure(figsize=(fig_w_instruct_pt*0.9, fig_h_instruct_pt*0.9))\n",
    "                plt.scatter(embeddings_2d_instruct_ft[:,0], embeddings_2d_instruct_ft[:,1], alpha=0.7, s=60)\n",
    "                for i,lbl in enumerate(plot_labels_instruct_ft): plt.annotate(lbl,(embeddings_2d_instruct_ft[i,0],embeddings_2d_instruct_ft[i,1]),textcoords=\"offset points\",xytext=(5,5),ha='center')\n",
    "                plt.title(f'Instruction-Aware ({INSTRUCT_MODEL_NAME}) Embeddings (\"Fine-tuned\" Stand-in) - PCA', fontsize=16)\n",
    "                plt.xlabel('PCA Comp 1'); plt.ylabel('PCA Comp 2'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "        else:\n",
    "            print(\"Not enough embeddings for 'fine-tuned' (stand-in) Instruction-Aware model analysis.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during 'fine-tuned' (stand-in) instruction model evaluation: {e}\")\n",
    "else:\n",
    "    print(\"Prerequisites for 'fine-tuned' (stand-in) instruction model evaluation not met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ff546",
   "metadata": {},
   "source": [
    "### Interpretation Notes for Fine-tuned Nomic Embed:\n",
    "* **Compare with Pre-trained Nomic:** The main goal is to see if MLM fine-tuning on \"Lumina Codex\" results in noticeable changes in how sentences (with the `search_document:` prefix) relate to each other. Does the embedding space become more specialized for \"Lumina Codex\" themes?\n",
    "* **Instruction Impact:** Remember that the primary way Nomic Embed adapts is via its input prefixes. This MLM fine-tuning is an *additional* layer of domain adaptation for its underlying representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
